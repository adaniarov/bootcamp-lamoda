{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c1f959b",
   "metadata": {},
   "source": [
    "# Полный baseline для Lamoda Bootcamp\n",
    "\n",
    "В этом ноутбуке реализован полный baseline для анализа отзывов и генерации тегов. Решение включает очистку текста, кластеризацию отзывов, извлечение ключевых аспектов и классификацию с использованием классических методов и более продвинутых моделей.\n",
    "\n",
    "> **Важно:** для работы русскоязычных трансформеров требуется интернет‑доступ для загрузки моделей. В данном окружении интернет ограничен, поэтому продвинутый модуль приведён в виде шаблона."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22940d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При необходимости установите дополнительные библиотеки.\n",
    "# !pip install -U scikit-learn nltk hdbscan sentence-transformers transformers pymorphy2\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41672d18",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Предполагается, что датасет находится в файле `lamoda_short.csv` и содержит колонку с текстом отзывов. Если файл не найден, создаётся небольшая демонстрационная выборка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec21e6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = '/home/oai/share/lamoda_short.csv'\n",
    "if os.path.exists(DATA_PATH):\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    possible_text_cols = ['review_text', 'text', 'comment', 'review']\n",
    "    text_col = next((c for c in possible_text_cols if c in df.columns), df.columns[0])\n",
    "    print(f'Загружено {len(df)} строк. Используемая колонка: {text_col}')\n",
    "else:\n",
    "    # Демонстрационные данные\n",
    "    data = {\n",
    "        'text': [\n",
    "            'Это красное платье из хлопка, очень понравилось!',\n",
    "            'Качество обуви ужасное, порвалась через неделю.',\n",
    "            'Отличный свитер, мягкий и тёплый.',\n",
    "            'Размер не совпадает, слишком мало.',\n",
    "            'Сумка выглядит дорого и стильно, рекомендую.',\n",
    "            'Доставка задержалась, сервис разочаровал.'\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    text_col = 'text'\n",
    "    print('Используется игрушечный набор данных.')\n",
    "\n",
    "# Приводим к единой колонке\n",
    "df = df[[text_col]].rename(columns={text_col: 'text'})\n",
    "df.dropna(inplace=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb87209c",
   "metadata": {},
   "source": [
    "## Предобработка текста\n",
    "\n",
    "Очистка включает приведение к нижнему регистру, удаление цифр, пунктуации и лишних пробелов. Можно расширить функцию лемматизацией, установив `pymorphy2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56f3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = str(text).lower()\n",
    "    # удаляем все символы кроме букв и пробелов\n",
    "    text = re.sub(r'[^а-яa-z\\s]', ' ', text)\n",
    "    # удаляем лишние пробелы\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# применяем очистку\n",
    "\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec326c78",
   "metadata": {},
   "source": [
    "## Классическая классификация\n",
    "\n",
    "Если в датасете есть колонка `label` с разметкой, можно обучить модель классификации. Используются словесные и символьные TF‑IDF и линейный SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e284d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'label' in df.columns:\n",
    "    X = df['clean_text']\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    word_vect = TfidfVectorizer(ngram_range=(1,2), max_features=50000, token_pattern=r'\b\\w+\b')\n",
    "    char_vect = TfidfVectorizer(analyzer='char', ngram_range=(3,5), max_features=50000)\n",
    "    vectorizer = FeatureUnion([('word', word_vect), ('char', char_vect)])\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    model = LinearSVC()\n",
    "    model.fit(X_train_vec, y_train)\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "    \n",
    "    print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "else:\n",
    "    print('В датасете нет колонки label — классификация пропущена.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bfe46",
   "metadata": {},
   "source": [
    "## Кластеризация отзывов и извлечение аспектов\n",
    "\n",
    "Для группировки отзывов используются TF‑IDF признаки и алгоритм KMeans. Для каждого кластера вычисляются наиболее частотные n‑граммы, которые можно использовать как шаблонные теги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b08548",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Векторизация для кластеризации\n",
    "vect = TfidfVectorizer(ngram_range=(1,2), max_features=30000, token_pattern=r'\b\\w+\b')\n",
    "X_vec = vect.fit_transform(df['clean_text'])\n",
    "\n",
    "# Определяем число кластеров: если отзывов мало, используем меньшее число\n",
    "n_clusters = 5 if len(df) > 50 else max(2, len(df)//2)\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(X_vec)\n",
    "\n",
    "# Функция для извлечения топ-тегов для кластера\n",
    "def get_top_terms(cluster_label, top_n=10):\n",
    "    mask = df['cluster'] == cluster_label\n",
    "    texts = df.loc[mask, 'clean_text']\n",
    "    local_vect = TfidfVectorizer(ngram_range=(1,2), max_features=5000, token_pattern=r'\b\\w+\b')\n",
    "    X_local = local_vect.fit_transform(texts)\n",
    "    sums = X_local.sum(axis=0)\n",
    "    freqs = [(term, sums[0, idx]) for term, idx in local_vect.vocabulary_.items()]\n",
    "    top_terms = sorted(freqs, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [term for term, _ in top_terms]\n",
    "\n",
    "# Выводим теги для каждого кластера\n",
    "cluster_tags = {}\n",
    "for cl in sorted(df['cluster'].unique()):\n",
    "    tags = get_top_terms(cl, top_n=8)\n",
    "    cluster_tags[cl] = tags\n",
    "    print(f'Кластер {cl}: {\", \".join(tags)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968018dc",
   "metadata": {},
   "source": [
    "## Шаблон продвинутой модели (BERT)\n",
    "\n",
    "Ниже приведён пример тонкой настройки русскоязычной модели BERT для классификации отзывов. Запустите эту ячейку в окружении с доступом к интернету и установленными `transformers` и `datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import Dataset\n",
    "# \n",
    "# model_name = 'DeepPavlov/rubert-base-cased'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# dataset = Dataset.from_pandas(df[['clean_text','label']])\n",
    "# \n",
    "# def preprocess_examples(batch):\n",
    "#     return tokenizer(batch['clean_text'], truncation=True, padding='max_length', max_length=128)\n",
    "# \n",
    "# tokenized_dataset = dataset.map(preprocess_examples, batched=True)\n",
    "# tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.2, stratify_by_column='label')\n",
    "# \n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(df['label'])))\n",
    "# \n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',\n",
    "#     evaluation_strategy='epoch',\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )\n",
    "# \n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset['train'],\n",
    "#     eval_dataset=tokenized_dataset['test'],\n",
    "# )\n",
    "# trainer.train()\n",
    "# trainer.evaluate()\n",
    "\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
