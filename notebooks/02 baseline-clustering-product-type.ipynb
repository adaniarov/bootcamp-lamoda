{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee9ca877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a.danyarov/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import json\n",
    "\n",
    "# NLP libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan  # Для кластеризации без фиксированного числа кластеров\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c8faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/all-mpnet-base-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING EMBEDDING MODEL\n",
      "================================================================================\n",
      "Trying to load model: paraphrase-multilingual-MiniLM-L12-v2...\n",
      "✗ paraphrase-multilingual-MiniLM-L12-v2: Not found in cache\n",
      "Trying to load model: all-MiniLM-L6-v2...\n",
      "✗ all-MiniLM-L6-v2: Not found in cache\n",
      "Trying to load model: all-mpnet-base-v2...\n",
      "✗ all-mpnet-base-v2: Not found in cache\n",
      "\n",
      "⚠ No sentence transformer models found in cache\n",
      "Using TF-IDF as fallback for embeddings\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROJECT SETUP & MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_DIR = Path().resolve()\n",
    "INTERIM_DIR = PROJECT_DIR / \"data/interim\"\n",
    "FILE_PATH = PROJECT_DIR / \"data/raw/lamoda_reviews.csv\"\n",
    "\n",
    "# Инициализация модели для embeddings\n",
    "print(\"=\"*80)\n",
    "print(\"INITIALIZING EMBEDDING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Пробуем загрузить модель из кеша\n",
    "model_candidates = [\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',  # Лучше для русского\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2',\n",
    "]\n",
    "\n",
    "sentence_model = None\n",
    "USE_TFIDF_FALLBACK = False\n",
    "\n",
    "for model_name in model_candidates:\n",
    "    try:\n",
    "        print(f\"Trying to load model: {model_name}...\")\n",
    "        sentence_model = SentenceTransformer(model_name, local_files_only=True)\n",
    "        print(f\"✓ Model '{model_name}' loaded successfully!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model_name}: Not found in cache\")\n",
    "        continue\n",
    "\n",
    "if sentence_model is None:\n",
    "    print(\"\\n⚠ No sentence transformer models found in cache\")\n",
    "    print(\"Using TF-IDF as fallback for embeddings\")\n",
    "    USE_TFIDF_FALLBACK = True\n",
    "else:\n",
    "    print(f\"\\n✓ Using model: {model_name}\")\n",
    "    print(f\"  Embedding dimension: {sentence_model.get_sentence_embedding_dimension()}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e69d487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ШАГ 0: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ ПО ТИПУ ПРОДУКТА\n",
      "================================================================================\n",
      "Loading data...\n",
      "Loaded 1774267 reviews\n",
      "Unique products: 254307\n",
      "Unique product types: 119\n",
      "Categories: ['Clothes' 'Accs' 'Shoes' 'Beauty_Accs' 'Home_Accs' 'Bags' 'Toys'\n",
      " 'Jewellery' nan]\n",
      "\n",
      "Preparing data structure by product type...\n",
      "✓ Prepared 112 product types with at least 10 reviews\n",
      "Using subset: 20 product types for testing\n",
      "\n",
      "Example product type structure:\n",
      "  Product Type: TEE-SHIRTS & POLOS\n",
      "  Category: Clothes\n",
      "  Reviews: 170470\n",
      "  Products: 23469\n",
      "  Sample review: Произведено в Турции. Качество хорошее и размеру соответствует....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>category</th>\n",
       "      <th>reviews</th>\n",
       "      <th>num_reviews</th>\n",
       "      <th>num_products</th>\n",
       "      <th>num_unique_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>TEE-SHIRTS &amp; POLOS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>[Произведено в Турции. Качество хорошее и разм...</td>\n",
       "      <td>170470</td>\n",
       "      <td>23469</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>SPORT SHOES</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>[Мужу понравились. , Кеды как кеды.купила за 1...</td>\n",
       "      <td>162149</td>\n",
       "      <td>20612</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>TROUSERS &amp; JUMPSUITS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>[Ничего плохого в них не вижу , Хорошие штаниш...</td>\n",
       "      <td>124705</td>\n",
       "      <td>18374</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>OUTWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>[Купили сыну на день рождения!Теплая, но не жа...</td>\n",
       "      <td>112425</td>\n",
       "      <td>18712</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>UNDERWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>[трусы огонь, Хорошие трусы, Хорошо все, Очень...</td>\n",
       "      <td>90187</td>\n",
       "      <td>11697</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             product_type category  \\\n",
       "99     TEE-SHIRTS & POLOS  Clothes   \n",
       "90            SPORT SHOES    Shoes   \n",
       "105  TROUSERS & JUMPSUITS  Clothes   \n",
       "73                OUTWEAR  Clothes   \n",
       "108             UNDERWEAR  Clothes   \n",
       "\n",
       "                                               reviews  num_reviews  \\\n",
       "99   [Произведено в Турции. Качество хорошее и разм...       170470   \n",
       "90   [Мужу понравились. , Кеды как кеды.купила за 1...       162149   \n",
       "105  [Ничего плохого в них не вижу , Хорошие штаниш...       124705   \n",
       "73   [Купили сыну на день рождения!Теплая, но не жа...       112425   \n",
       "108  [трусы огонь, Хорошие трусы, Хорошо все, Очень...        90187   \n",
       "\n",
       "     num_products  num_unique_names  \n",
       "99          23469                56  \n",
       "90          20612                32  \n",
       "105         18374                49  \n",
       "73          18712                94  \n",
       "108         11697               105  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 0: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ ПО ТИПУ ПРОДУКТА\n",
    "# ============================================================================\n",
    "# Объекты: product_type, category, reviews (объединенные по типу продукта)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 0: ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ ПО ТИПУ ПРОДУКТА\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Загружаем данные\n",
    "print(\"Loading data...\")\n",
    "df_raw = pd.read_csv(FILE_PATH)\n",
    "print(f\"Loaded {len(df_raw)} reviews\")\n",
    "print(f\"Unique products: {df_raw['product_sku'].nunique()}\")\n",
    "print(f\"Unique product types: {df_raw['good_subtype'].nunique()}\")\n",
    "print(f\"Categories: {df_raw['good_type'].unique()}\")\n",
    "\n",
    "# Группируем по ТИПУ ПРОДУКТА (product_type) вместо по продукту\n",
    "print(\"\\nPreparing data structure by product type...\")\n",
    "product_types_data = []\n",
    "\n",
    "for product_type, group in df_raw.groupby('good_subtype'):\n",
    "    if pd.isna(product_type):\n",
    "        continue\n",
    "    \n",
    "    # Собираем все отзывы для всех продуктов этого типа\n",
    "    reviews = group['comment_text'].dropna().tolist()\n",
    "    \n",
    "    if len(reviews) < 10:  # Минимум 10 отзывов для анализа типа продукта\n",
    "        continue\n",
    "    \n",
    "    # Получаем категорию (берем самую частую)\n",
    "    category = group['good_type'].mode()[0] if 'good_type' in group.columns else None\n",
    "    \n",
    "    # Собираем информацию о продуктах этого типа\n",
    "    unique_skus = group['product_sku'].nunique()\n",
    "    unique_names = group['name'].nunique()\n",
    "    \n",
    "    product_types_data.append({\n",
    "        'product_type': product_type,\n",
    "        'category': category,\n",
    "        'reviews': reviews,\n",
    "        'num_reviews': len(reviews),\n",
    "        'num_products': unique_skus,\n",
    "        'num_unique_names': unique_names\n",
    "    })\n",
    "\n",
    "# Создаем DataFrame\n",
    "df_product_types = pd.DataFrame(product_types_data)\n",
    "print(f\"✓ Prepared {len(df_product_types)} product types with at least 10 reviews\")\n",
    "\n",
    "# Для тестирования можно использовать подмножество\n",
    "USE_SUBSET = True\n",
    "if USE_SUBSET:\n",
    "    # Берем топ-20 типов продуктов по количеству отзывов\n",
    "    df_product_types = df_product_types.nlargest(20, 'num_reviews')\n",
    "    print(f\"Using subset: {len(df_product_types)} product types for testing\")\n",
    "\n",
    "print(f\"\\nExample product type structure:\")\n",
    "example = df_product_types.iloc[0]\n",
    "print(f\"  Product Type: {example['product_type']}\")\n",
    "print(f\"  Category: {example['category']}\")\n",
    "print(f\"  Reviews: {example['num_reviews']}\")\n",
    "print(f\"  Products: {example['num_products']}\")\n",
    "print(f\"  Sample review: {example['reviews'][0][:80]}...\")\n",
    "\n",
    "df_product_types.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec2cc64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ШАГ 1: ОЧИСТКА ТЕКСТОВ\n",
      "================================================================================\n",
      "Cleaning reviews...\n",
      "✓ Cleaned reviews for 20 product types\n",
      "  Total reviews before: 1396425\n",
      "  Total reviews after: 1396405\n",
      "\n",
      "Example (Product Type: TEE-SHIRTS & POLOS):\n",
      "  Original: Произведено в Турции. Качество хорошее и размеру соответствует....\n",
      "  Cleaned:  произведено в турции. качество хорошее и размеру соответствует....\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>category</th>\n",
       "      <th>num_clean_reviews</th>\n",
       "      <th>num_products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>TEE-SHIRTS &amp; POLOS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>170470</td>\n",
       "      <td>23469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>SPORT SHOES</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>162145</td>\n",
       "      <td>20612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>TROUSERS &amp; JUMPSUITS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>124703</td>\n",
       "      <td>18374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>OUTWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>112421</td>\n",
       "      <td>18712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>UNDERWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>90185</td>\n",
       "      <td>11697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             product_type category  num_clean_reviews  num_products\n",
       "99     TEE-SHIRTS & POLOS  Clothes             170470         23469\n",
       "90            SPORT SHOES    Shoes             162145         20612\n",
       "105  TROUSERS & JUMPSUITS  Clothes             124703         18374\n",
       "73                OUTWEAR  Clothes             112421         18712\n",
       "108             UNDERWEAR  Clothes              90185         11697"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 1: ОЧИСТКА ТЕКСТОВ\n",
    "# ============================================================================\n",
    "# Вход: reviews: List[str]\n",
    "# Выход: clean_reviews: List[str]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 1: ОЧИСТКА ТЕКСТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def clean_text_soft(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Мягкая очистка текста (feature cleaning, не feature engineering):\n",
    "    - Приводим к нижнему регистру\n",
    "    - Убираем HTML теги\n",
    "    - Убираем лишние символы\n",
    "    - НЕ удаляем слова по смыслу\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Убираем HTML теги\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Убираем множественные пробелы и переносы строк\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Убираем управляющие символы\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    \n",
    "    # Приводим к нижнему регистру\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Применяем очистку ко всем отзывам всех типов продуктов\n",
    "print(\"Cleaning reviews...\")\n",
    "df_product_types['clean_reviews'] = df_product_types['reviews'].apply(\n",
    "    lambda reviews: [clean_text_soft(r) for r in reviews if clean_text_soft(r)]\n",
    ")\n",
    "\n",
    "# Удаляем типы продуктов без отзывов после очистки\n",
    "df_product_types = df_product_types[df_product_types['clean_reviews'].apply(len) >= 10].copy()\n",
    "df_product_types['num_clean_reviews'] = df_product_types['clean_reviews'].apply(len)\n",
    "\n",
    "print(f\"✓ Cleaned reviews for {len(df_product_types)} product types\")\n",
    "print(f\"  Total reviews before: {df_product_types['num_reviews'].sum()}\")\n",
    "print(f\"  Total reviews after: {df_product_types['num_clean_reviews'].sum()}\")\n",
    "\n",
    "# Показываем пример\n",
    "example = df_product_types.iloc[0]\n",
    "print(f\"\\nExample (Product Type: {example['product_type']}):\")\n",
    "print(f\"  Original: {example['reviews'][0][:100]}...\")\n",
    "print(f\"  Cleaned:  {example['clean_reviews'][0][:100]}...\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'num_clean_reviews', 'num_products']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "396ca5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ШАГ 2: ГЕНЕРАЦИЯ EMBEDDINGS\n",
      "================================================================================\n",
      "Processing product types...\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (170470, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (162145, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (124703, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (112421, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (90185, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (89737, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (87520, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (80475, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (73908, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (52810, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (44402, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (40603, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (35556, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (34966, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (34859, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (34701, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (33621, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (33174, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (31574, 384)\n",
      "Using TF-IDF embeddings (fallback mode)...\n",
      "✓ Generated TF-IDF embeddings shape: (28575, 384)\n",
      "\n",
      "✓ Generated embeddings for 20 product types\n",
      "  Embedding dimension: 384\n",
      "\n",
      "Example (Product Type: TEE-SHIRTS & POLOS):\n",
      "  Reviews: 170470\n",
      "  Embeddings shape: (170470, 384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>category</th>\n",
       "      <th>num_clean_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>TEE-SHIRTS &amp; POLOS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>170470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>SPORT SHOES</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>162145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>TROUSERS &amp; JUMPSUITS</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>124703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>OUTWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>112421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>UNDERWEAR</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>90185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             product_type category  num_clean_reviews\n",
       "99     TEE-SHIRTS & POLOS  Clothes             170470\n",
       "90            SPORT SHOES    Shoes             162145\n",
       "105  TROUSERS & JUMPSUITS  Clothes             124703\n",
       "73                OUTWEAR  Clothes             112421\n",
       "108             UNDERWEAR  Clothes              90185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 2: ПРЕОБРАЗОВАНИЕ ОТЗЫВОВ В ЧИСЛОВЫЕ ВЕКТОРЫ (EMBEDDINGS)\n",
    "# ============================================================================\n",
    "# Вход: clean_reviews: List[str]\n",
    "# Выход: review_embeddings: ndarray shape (n_reviews, embedding_dim)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 2: ГЕНЕРАЦИЯ EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def get_embeddings(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Преобразование текстов в числовые векторы фиксированной длины\n",
    "    FeatureExtractor(text) → R^embedding_dim\n",
    "    \"\"\"\n",
    "    if USE_TFIDF_FALLBACK:\n",
    "        print(f\"Using TF-IDF embeddings (fallback mode)...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=384,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=1,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        embeddings = vectorizer.fit_transform(texts).toarray()\n",
    "        print(f\"✓ Generated TF-IDF embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    else:\n",
    "        if sentence_model is None:\n",
    "            raise ValueError(\"Sentence transformer model is not loaded!\")\n",
    "        \n",
    "        print(f\"Generating sentence transformer embeddings...\")\n",
    "        embeddings = sentence_model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        print(f\"✓ Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Генерируем embeddings для каждого типа продукта\n",
    "print(\"Processing product types...\")\n",
    "all_embeddings = []\n",
    "\n",
    "for idx, row in df_product_types.iterrows():\n",
    "    clean_reviews = row['clean_reviews']\n",
    "    \n",
    "    # Получаем embeddings для всех отзывов типа продукта\n",
    "    embeddings = get_embeddings(clean_reviews)\n",
    "    all_embeddings.append(embeddings)\n",
    "\n",
    "# Сохраняем embeddings в DataFrame\n",
    "df_product_types['review_embeddings'] = all_embeddings\n",
    "\n",
    "print(f\"\\n✓ Generated embeddings for {len(df_product_types)} product types\")\n",
    "print(f\"  Embedding dimension: {df_product_types['review_embeddings'].iloc[0].shape[1]}\")\n",
    "\n",
    "# Показываем пример\n",
    "example = df_product_types.iloc[0]\n",
    "print(f\"\\nExample (Product Type: {example['product_type']}):\")\n",
    "print(f\"  Reviews: {len(example['clean_reviews'])}\")\n",
    "print(f\"  Embeddings shape: {example['review_embeddings'].shape}\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'num_clean_reviews']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45ca32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ШАГ 3: КЛАСТЕРИЗАЦИЯ ОТЗЫВОВ\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mШАГ 3: КЛАСТЕРИЗАЦИЯ ОТЗЫВОВ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcluster_reviews\u001b[39m(embeddings: \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mndarray, min_cluster_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Кластеризация отзывов БЕЗ фиксированного числа кластеров\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Используем HDBSCAN для автоматического определения числа кластеров\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Возвращает массив cluster_id для каждого отзыва (-1 = шум/outlier)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(embeddings)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 3: КЛАСТЕРИЗАЦИЯ ОТЗЫВОВ ПО СМЫСЛУ\n",
    "# ============================================================================\n",
    "# Вход: review_embeddings: ndarray\n",
    "# Выход: clusters: List[int]  # cluster_id для каждого отзыва\n",
    "# Применяем кластеризацию БЕЗ заранее заданного числа кластеров\n",
    "# Цель: сгруппировать отзывы по тому, о чём они (материал, комфорт, доставка, качество)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 3: КЛАСТЕРИЗАЦИЯ ОТЗЫВОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def cluster_reviews(embeddings: np.ndarray, min_cluster_size: int = 5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Кластеризация отзывов БЕЗ фиксированного числа кластеров\n",
    "    Используем HDBSCAN для автоматического определения числа кластеров\n",
    "    Возвращает массив cluster_id для каждого отзыва (-1 = шум/outlier)\n",
    "    \"\"\"\n",
    "    n_samples = len(embeddings)\n",
    "    \n",
    "    if n_samples < min_cluster_size:\n",
    "        # Если отзывов слишком мало, все в один кластер\n",
    "        return np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    # Используем HDBSCAN для автоматической кластеризации\n",
    "    # min_cluster_size - минимальный размер кластера (больше для типов продуктов)\n",
    "    # min_samples - минимальное количество соседей для core point\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=2,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'  # Excess of Mass\n",
    "    )\n",
    "    \n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "    \n",
    "    # Если все отзывы стали шумом, используем KMeans как fallback\n",
    "    if np.all(cluster_labels == -1):\n",
    "        n_clusters = min(5, n_samples // 10)\n",
    "        if n_clusters > 0:\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "# Применяем кластеризацию для каждого типа продукта\n",
    "print(\"Clustering reviews for each product type...\")\n",
    "all_clusters = []\n",
    "\n",
    "for idx, row in df_product_types.iterrows():\n",
    "    embeddings = row['review_embeddings']\n",
    "    clean_reviews = row['clean_reviews']\n",
    "    \n",
    "    # Кластеризуем отзывы типа продукта\n",
    "    # Используем больший min_cluster_size, т.к. у нас больше данных\n",
    "    cluster_labels = cluster_reviews(embeddings, min_cluster_size=5)\n",
    "    \n",
    "    all_clusters.append(cluster_labels)\n",
    "    \n",
    "    # Показываем статистику для первых нескольких типов продуктов\n",
    "    if idx < 3:\n",
    "        unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "        print(f\"\\n  Product Type: {row['product_type']}\")\n",
    "        print(f\"    Reviews: {len(clean_reviews)}\")\n",
    "        print(f\"    Clusters: {len(unique[unique >= 0])} (noise: {np.sum(cluster_labels == -1)})\")\n",
    "        for cluster_id, count in zip(unique, counts):\n",
    "            if cluster_id >= 0:\n",
    "                print(f\"      Cluster {cluster_id}: {count} reviews\")\n",
    "\n",
    "# Сохраняем кластеры\n",
    "df_product_types['clusters'] = all_clusters\n",
    "\n",
    "print(f\"\\n✓ Clustered reviews for {len(df_product_types)} product types\")\n",
    "\n",
    "# Показываем общую статистику\n",
    "total_clusters = sum(len(np.unique(c[c >= 0])) for c in all_clusters)\n",
    "total_noise = sum(np.sum(c == -1) for c in all_clusters)\n",
    "print(f\"  Total clusters found: {total_clusters}\")\n",
    "print(f\"  Total noise/outliers: {total_noise}\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'num_clean_reviews']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dc6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 4: РАБОТА С КЛАСТЕРАМИ (НЕ С ОТЗЫВАМИ)\n",
    "# ============================================================================\n",
    "# Объект меняется: теперь основная единица анализа = Cluster\n",
    "# Cluster = набор отзывов с общим смыслом\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 4: ГРУППИРОВКА ПО КЛАСТЕРАМ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def group_reviews_by_clusters(\n",
    "    clean_reviews: List[str],\n",
    "    clusters: np.ndarray,\n",
    "    embeddings: np.ndarray\n",
    ") -> Dict[int, Dict]:\n",
    "    \"\"\"\n",
    "    Группировка отзывов по кластерам\n",
    "    Возвращает словарь {cluster_id: {'texts': [...], 'embeddings': [...]}}\n",
    "    \"\"\"\n",
    "    cluster_groups = {}\n",
    "    \n",
    "    for cluster_id in np.unique(clusters):\n",
    "        if cluster_id == -1:\n",
    "            # Пропускаем шум\n",
    "            continue\n",
    "        \n",
    "        # Находим индексы отзывов в этом кластере\n",
    "        cluster_indices = np.where(clusters == cluster_id)[0]\n",
    "        \n",
    "        cluster_groups[cluster_id] = {\n",
    "            'texts': [clean_reviews[i] for i in cluster_indices],\n",
    "            'embeddings': embeddings[cluster_indices]\n",
    "        }\n",
    "    \n",
    "    return cluster_groups\n",
    "\n",
    "\n",
    "# Группируем отзывы по кластерам для каждого типа продукта\n",
    "print(\"Grouping reviews by clusters...\")\n",
    "all_cluster_groups = []\n",
    "\n",
    "for idx, row in df_product_types.iterrows():\n",
    "    clean_reviews = row['clean_reviews']\n",
    "    clusters = row['clusters']\n",
    "    embeddings = row['review_embeddings']\n",
    "    \n",
    "    # Группируем по кластерам\n",
    "    cluster_groups = group_reviews_by_clusters(clean_reviews, clusters, embeddings)\n",
    "    all_cluster_groups.append(cluster_groups)\n",
    "    \n",
    "    # Показываем примеры для первых типов продуктов\n",
    "    if idx < 3:\n",
    "        print(f\"\\n  Product Type: {row['product_type']} ({row['category']}):\")\n",
    "        for cluster_id, group in list(cluster_groups.items())[:3]:\n",
    "            print(f\"    Cluster {cluster_id}: {len(group['texts'])} reviews\")\n",
    "            print(f\"      Example: {group['texts'][0][:80]}...\")\n",
    "\n",
    "# Сохраняем группировку\n",
    "df_product_types['cluster_groups'] = all_cluster_groups\n",
    "\n",
    "print(f\"\\n✓ Grouped reviews into clusters for {len(df_product_types)} product types\")\n",
    "\n",
    "# Статистика\n",
    "total_clusters = sum(len(groups) for groups in all_cluster_groups)\n",
    "avg_cluster_size = np.mean([\n",
    "    len(texts) \n",
    "    for groups in all_cluster_groups \n",
    "    for cluster_id, group in groups.items() \n",
    "    for texts in [group['texts']]\n",
    "])\n",
    "print(f\"  Total clusters: {total_clusters}\")\n",
    "print(f\"  Average cluster size: {avg_cluster_size:.2f} reviews\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'num_clean_reviews']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa3ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 5: ИЗВЛЕЧЕНИЕ АСПЕКТОВ ИЗ КЛАСТЕРОВ\n",
    "# ============================================================================\n",
    "# Вход: texts_in_cluster: List[str]\n",
    "# Выход: raw_aspects: List[str]\n",
    "# Аспект = формулировка свойства товара, которое регулярно упоминается в кластере\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 5: ИЗВЛЕЧЕНИЕ АСПЕКТОВ ИЗ КЛАСТЕРОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def extract_aspects_from_cluster(\n",
    "    cluster_texts: List[str],\n",
    "    min_word_freq: int = 3,\n",
    "    max_aspects: int = 5\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Извлечение аспектов из кластера\n",
    "    Находим top-features (слова/фразы), которые отличают этот кластер от других\n",
    "    \"\"\"\n",
    "    if len(cluster_texts) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Объединяем все тексты кластера\n",
    "    all_text = ' '.join(cluster_texts)\n",
    "    \n",
    "    # Разбиваем на слова\n",
    "    words = re.findall(r'\\b[а-яёa-z]{3,}\\b', all_text.lower())\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Подсчитываем частоту слов\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Фильтруем слишком частые (стоп-слова) и слишком редкие\n",
    "    filtered_words = {\n",
    "        word: count \n",
    "        for word, count in word_counts.items() \n",
    "        if count >= min_word_freq and count < len(cluster_texts) * 0.9\n",
    "    }\n",
    "    \n",
    "    # Сортируем по частоте и берем топ\n",
    "    top_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:max_aspects]\n",
    "    \n",
    "    # Формируем аспекты из топ-слов\n",
    "    aspects = []\n",
    "    for word, count in top_words:\n",
    "        # Ищем предложения, содержащие это слово\n",
    "        for text in cluster_texts:\n",
    "            if word in text.lower():\n",
    "                # Берем короткую фразу вокруг слова\n",
    "                words_in_text = text.split()\n",
    "                if word in [w.lower() for w in words_in_text]:\n",
    "                    # Берем до 5 слов вокруг ключевого слова\n",
    "                    word_idx = [i for i, w in enumerate(words_in_text) if word in w.lower()]\n",
    "                    if word_idx:\n",
    "                        start = max(0, word_idx[0] - 2)\n",
    "                        end = min(len(words_in_text), word_idx[0] + 3)\n",
    "                        phrase = ' '.join(words_in_text[start:end])\n",
    "                        if len(phrase) > 10 and phrase not in aspects:\n",
    "                            aspects.append(phrase)\n",
    "                            break\n",
    "        \n",
    "        if len(aspects) >= max_aspects:\n",
    "            break\n",
    "    \n",
    "    # Если не нашли фразы, возвращаем просто топ-слова\n",
    "    if len(aspects) == 0:\n",
    "        aspects = [word for word, _ in top_words[:max_aspects]]\n",
    "    \n",
    "    return aspects[:max_aspects]\n",
    "\n",
    "\n",
    "# Извлекаем аспекты для каждого кластера каждого типа продукта\n",
    "print(\"Extracting aspects from clusters...\")\n",
    "all_product_type_aspects = []\n",
    "\n",
    "for idx, row in df_product_types.iterrows():\n",
    "    cluster_groups = row['cluster_groups']\n",
    "    \n",
    "    product_type_aspects = {}\n",
    "    \n",
    "    for cluster_id, group in cluster_groups.items():\n",
    "        cluster_texts = group['texts']\n",
    "        \n",
    "        # Извлекаем аспекты из кластера\n",
    "        aspects = extract_aspects_from_cluster(cluster_texts, min_word_freq=3, max_aspects=5)\n",
    "        \n",
    "        if aspects:\n",
    "            product_type_aspects[cluster_id] = aspects\n",
    "    \n",
    "    all_product_type_aspects.append(product_type_aspects)\n",
    "    \n",
    "    # Показываем примеры для первых типов продуктов\n",
    "    if idx < 3 and product_type_aspects:\n",
    "        print(f\"\\n  Product Type: {row['product_type']} ({row['category']}):\")\n",
    "        for cluster_id, aspects in list(product_type_aspects.items())[:2]:\n",
    "            print(f\"    Cluster {cluster_id}: {aspects}\")\n",
    "\n",
    "# Сохраняем аспекты\n",
    "df_product_types['raw_aspects'] = all_product_type_aspects\n",
    "\n",
    "print(f\"\\n✓ Extracted aspects for {len(df_product_types)} product types\")\n",
    "\n",
    "# Статистика\n",
    "total_aspects = sum(len(aspects) for aspects in all_product_type_aspects)\n",
    "print(f\"  Total aspects extracted: {total_aspects}\")\n",
    "print(f\"  Average aspects per product type: {total_aspects / len(df_product_types):.2f}\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'raw_aspects']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35229412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ШАГ 6: ФИЛЬТРАЦИЯ АСПЕКТОВ С УЧЕТОМ КАТЕГОРИИ\n",
    "# ============================================================================\n",
    "# Вход: raw_aspects + category\n",
    "# Выход: filtered_aspects: List[str]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ШАГ 6: ФИЛЬТРАЦИЯ АСПЕКТОВ (CATEGORY-AWARE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Словари запрещенных и приоритетных аспектов для разных категорий\n",
    "FORBIDDEN_ASPECTS = {\n",
    "    'Clothes': ['доставка', 'упаковка'],\n",
    "    'Shoes': ['доставка', 'упаковка'],\n",
    "    'Accs': ['доставка', 'упаковка'],\n",
    "    'Beauty_Accs': ['размер'],\n",
    "}\n",
    "\n",
    "PRIORITY_ASPECTS = {\n",
    "    'Clothes': ['размер', 'материал', 'качество', 'комфорт'],\n",
    "    'Shoes': ['размер', 'комфорт', 'качество'],\n",
    "    'Accs': ['качество', 'материал', 'внешний вид'],\n",
    "    'Beauty_Accs': ['качество', 'эффект', 'аромат'],\n",
    "}\n",
    "\n",
    "def filter_aspects(\n",
    "    raw_aspects: Dict[int, List[str]],\n",
    "    category: Optional[str]\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Фильтрация и приоритизация аспектов с учетом категории\n",
    "    \"\"\"\n",
    "    # Собираем все аспекты в один список\n",
    "    all_aspects = []\n",
    "    for cluster_id, aspects in raw_aspects.items():\n",
    "        all_aspects.extend(aspects)\n",
    "    \n",
    "    if len(all_aspects) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Фильтруем запрещенные аспекты\n",
    "    forbidden = FORBIDDEN_ASPECTS.get(category, [])\n",
    "    filtered = [\n",
    "        aspect for aspect in all_aspects\n",
    "        if not any(forbidden_word in aspect.lower() for forbidden_word in forbidden)\n",
    "    ]\n",
    "    \n",
    "    # Если после фильтрации ничего не осталось, возвращаем исходные\n",
    "    if len(filtered) == 0:\n",
    "        filtered = all_aspects\n",
    "    \n",
    "    # Приоритизируем важные аспекты\n",
    "    priority = PRIORITY_ASPECTS.get(category, [])\n",
    "    priority_aspects = []\n",
    "    other_aspects = []\n",
    "    \n",
    "    for aspect in filtered:\n",
    "        is_priority = any(priority_word in aspect.lower() for priority_word in priority)\n",
    "        if is_priority:\n",
    "            priority_aspects.append(aspect)\n",
    "        else:\n",
    "            other_aspects.append(aspect)\n",
    "    \n",
    "    # Сортируем: сначала приоритетные, потом остальные\n",
    "    result = priority_aspects + other_aspects\n",
    "    \n",
    "    # Ограничиваем количество (максимум 8 аспектов для типа продукта)\n",
    "    return result[:8]\n",
    "\n",
    "\n",
    "# Применяем фильтрацию для каждого типа продукта\n",
    "print(\"Filtering aspects by category...\")\n",
    "all_filtered_aspects = []\n",
    "\n",
    "for idx, row in df_product_types.iterrows():\n",
    "    raw_aspects = row['raw_aspects']\n",
    "    category = row['category']\n",
    "    \n",
    "    # Фильтруем аспекты\n",
    "    filtered = filter_aspects(raw_aspects, category)\n",
    "    all_filtered_aspects.append(filtered)\n",
    "    \n",
    "    # Показываем примеры для первых типов продуктов\n",
    "    if idx < 3:\n",
    "        print(f\"\\n  Product Type: {row['product_type']} ({category}):\")\n",
    "        print(f\"    Raw aspects: {sum(len(a) for a in raw_aspects.values())}\")\n",
    "        print(f\"    Filtered aspects ({len(filtered)}): {filtered[:5]}...\")\n",
    "\n",
    "# Сохраняем отфильтрованные аспекты\n",
    "df_product_types['filtered_aspects'] = all_filtered_aspects\n",
    "df_product_types['num_aspects'] = [len(a) for a in all_filtered_aspects]\n",
    "\n",
    "print(f\"\\n✓ Filtered aspects for {len(df_product_types)} product types\")\n",
    "\n",
    "# Статистика\n",
    "total_aspects = sum(len(a) for a in all_filtered_aspects)\n",
    "print(f\"  Total filtered aspects: {total_aspects}\")\n",
    "print(f\"  Average aspects per product type: {total_aspects / len(df_product_types):.2f}\")\n",
    "\n",
    "df_product_types[['product_type', 'category', 'num_aspects', 'filtered_aspects']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0937e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# РЕЗУЛЬТАТЫ И СТАТИСТИКА\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"РЕЗУЛЬТАТЫ ПАЙПЛАЙНА\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Формируем финальный DataFrame с результатами\n",
    "results_df = df_product_types[[\n",
    "    'product_type', 'category', \n",
    "    'num_reviews', 'num_products', 'num_aspects', 'filtered_aspects'\n",
    "]].copy()\n",
    "\n",
    "print(f\"\\n✓ Final results for {len(results_df)} product types\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Product types with aspects: {(results_df['num_aspects'] > 0).sum()}\")\n",
    "print(f\"  Average aspects per product type: {results_df['num_aspects'].mean():.2f}\")\n",
    "print(f\"  Min aspects: {results_df['num_aspects'].min()}\")\n",
    "print(f\"  Max aspects: {results_df['num_aspects'].max()}\")\n",
    "print(f\"  Median aspects: {results_df['num_aspects'].median():.2f}\")\n",
    "\n",
    "# Дополнительная статистика\n",
    "print(f\"\\n  Total reviews processed: {results_df['num_reviews'].sum():,}\")\n",
    "print(f\"  Total products covered: {results_df['num_products'].sum():,}\")\n",
    "print(f\"  Average reviews per product type: {results_df['num_reviews'].mean():.0f}\")\n",
    "print(f\"  Average products per product type: {results_df['num_products'].mean():.1f}\")\n",
    "\n",
    "# Статистика по кластерам\n",
    "print(f\"\\n  Clustering statistics:\")\n",
    "total_clusters = sum(len(groups) for groups in all_cluster_groups)\n",
    "total_noise = sum(np.sum(c == -1) for c in all_clusters)\n",
    "total_reviews_clustered = sum(len(c) for c in all_clusters)\n",
    "print(f\"    Total clusters: {total_clusters}\")\n",
    "print(f\"    Average clusters per product type: {total_clusters / len(results_df):.2f}\")\n",
    "print(f\"    Noise/outliers: {total_noise} ({100*total_noise/total_reviews_clustered:.1f}%)\")\n",
    "\n",
    "# Статистика по категориям\n",
    "print(f\"\\n  Statistics by category:\")\n",
    "category_stats = results_df.groupby('category').agg({\n",
    "    'num_aspects': ['count', 'mean', 'sum'],\n",
    "    'num_reviews': 'sum',\n",
    "    'num_products': 'sum'\n",
    "}).round(2)\n",
    "print(category_stats)\n",
    "\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a977eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ПРИМЕРЫ РЕЗУЛЬТАТОВ ПО КАТЕГОРИЯМ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXAMPLES BY CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in results_df['category'].dropna().unique()[:3]:\n",
    "    cat_types = results_df[results_df['category'] == category].head(3)\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in cat_types.iterrows():\n",
    "        print(f\"\\n  Product Type: {row['product_type']}\")\n",
    "        print(f\"  Reviews: {row['num_reviews']:,}, Products: {row['num_products']}, Aspects: {row['num_aspects']}\")\n",
    "        if row['filtered_aspects']:\n",
    "            print(f\"  Aspects:\")\n",
    "            for i, aspect in enumerate(row['filtered_aspects'][:6], 1):\n",
    "                print(f\"    {i}. {aspect}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ВИЗУАЛИЗАЦИЯ СТАТИСТИК\n",
    "# ============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Распределение количества аспектов\n",
    "axes[0, 0].hist(results_df['num_aspects'], bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Number of Aspects')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Aspects per Product Type')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Количество аспектов по категориям\n",
    "category_aspects = results_df.groupby('category')['num_aspects'].mean().sort_values(ascending=False)\n",
    "axes[0, 1].barh(category_aspects.index, category_aspects.values)\n",
    "axes[0, 1].set_xlabel('Average Number of Aspects')\n",
    "axes[0, 1].set_title('Average Aspects by Category')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Распределение количества отзывов\n",
    "axes[1, 0].hist(results_df['num_reviews'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Number of Reviews')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Reviews per Product Type')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Соотношение отзывов и аспектов\n",
    "axes[1, 1].scatter(results_df['num_reviews'], results_df['num_aspects'], alpha=0.6)\n",
    "axes[1, 1].set_xlabel('Number of Reviews')\n",
    "axes[1, 1].set_ylabel('Number of Aspects')\n",
    "axes[1, 1].set_title('Reviews vs Aspects')\n",
    "axes[1, 1].set_xscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Дополнительная статистика\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ДЕТАЛЬНАЯ СТАТИСТИКА\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nРаспределение аспектов:\")\n",
    "print(results_df['num_aspects'].describe())\n",
    "\n",
    "print(f\"\\nТоп-10 типов продуктов по количеству отзывов:\")\n",
    "top_reviews = results_df.nlargest(10, 'num_reviews')[['product_type', 'category', 'num_reviews', 'num_aspects']]\n",
    "print(top_reviews.to_string(index=False))\n",
    "\n",
    "print(f\"\\nТоп-10 типов продуктов по количеству аспектов:\")\n",
    "top_aspects = results_df.nlargest(10, 'num_aspects')[['product_type', 'category', 'num_reviews', 'num_aspects']]\n",
    "print(top_aspects.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ЭКСПОРТ РЕЗУЛЬТАТОВ\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ЭКСПОРТ РЕЗУЛЬТАТОВ\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Подготавливаем данные для экспорта\n",
    "export_df = results_df.copy()\n",
    "export_df['aspects'] = export_df['filtered_aspects'].apply(\n",
    "    lambda x: ' | '.join(x) if isinstance(x, list) else ''\n",
    ")\n",
    "\n",
    "# Сохраняем в CSV\n",
    "output_csv = INTERIM_DIR / \"product_type_aspects.csv\"\n",
    "export_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "print(f\"✓ Results saved to CSV: {output_csv}\")\n",
    "\n",
    "# Сохраняем в JSON (с полными списками аспектов)\n",
    "output_json = INTERIM_DIR / \"product_type_aspects.json\"\n",
    "results_df.to_json(output_json, orient='records', force_ascii=False, indent=2)\n",
    "print(f\"✓ Results saved to JSON: {output_json}\")\n",
    "\n",
    "# Финальная статистика\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ФИНАЛЬНАЯ СТАТИСТИКА\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total product types processed: {len(results_df)}\")\n",
    "print(f\"Product types with aspects: {(results_df['num_aspects'] > 0).sum()}\")\n",
    "print(f\"Average aspects per product type: {results_df['num_aspects'].mean():.2f}\")\n",
    "print(f\"\\nAspects distribution:\")\n",
    "print(results_df['num_aspects'].value_counts().sort_index())\n",
    "print(f\"\\nCategories distribution:\")\n",
    "print(results_df['category'].value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ПАЙПЛАЙН ЗАВЕРШЕН\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nСтруктура пайплайна:\")\n",
    "print(\"  0. Исходные данные: product_type, category, reviews (объединенные по типу)\")\n",
    "print(\"  1. Очистка текстов (мягкие правила)\")\n",
    "print(\"  2. Преобразование в числовые векторы (embeddings)\")\n",
    "print(\"  3. Кластеризация отзывов по смыслу (без фиксированного числа кластеров)\")\n",
    "print(\"  4. Работа с кластерами (группировка отзывов)\")\n",
    "print(\"  5. Извлечение аспектов из кластера (поиск частых слов/фраз)\")\n",
    "print(\"  6. Фильтрация аспектов с учетом category\")\n",
    "print(\"\\nРезультаты сохранены в:\")\n",
    "print(f\"  - {output_csv}\")\n",
    "print(f\"  - {output_json}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8632a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamoda-bootcamp-9UBAvNEw-py3.10 (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
