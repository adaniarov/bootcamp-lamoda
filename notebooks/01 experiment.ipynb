{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b31dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# NLP libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4389009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/all-mpnet-base-v2. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2. Creating a new one with mean pooling.\n",
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model...\n",
      "Trying different models...\n",
      "\n",
      "Trying model: all-MiniLM-L6-v2...\n",
      "✗ all-MiniLM-L6-v2: OSError\n",
      "\n",
      "Trying model: all-mpnet-base-v2...\n",
      "✗ all-mpnet-base-v2: OSError\n",
      "\n",
      "Trying model: paraphrase-multilingual-MiniLM-L12-v2...\n",
      "✗ paraphrase-multilingual-MiniLM-L12-v2: OSError\n",
      "\n",
      "Trying model: sentence-transformers/all-MiniLM-L6-v2...\n",
      "✗ sentence-transformers/all-MiniLM-L6-v2: OSError\n",
      "\n",
      "No models found in cache. Trying to download 'all-MiniLM-L6-v2'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1017)')))\"), '(Request ID: a02a200d-5308-477a-9455-83a29fd6fdc0)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1017)')))\"), '(Request ID: 3edc9ac1-613b-4fde-ad61-b85f32816f23)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1017)')))\"), '(Request ID: a6d98b2f-f447-4dc9-8892-6632a8c55d0d)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNo models found in cache. Trying to download \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m     sentence_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     loaded_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m downloaded and loaded!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:309\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[1;32m    308\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[0;32m--> 309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m \u001b[43mis_sentence_transformer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    317\u001b[0m     has_modules\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    326\u001b[0m ):\n\u001b[1;32m    327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    328\u001b[0m         model_name_or_path,\n\u001b[1;32m    329\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    337\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/sentence_transformers/util/file_io.py:53\u001b[0m, in \u001b[0;36mis_sentence_transformer_model\u001b[0;34m(model_name_or_path, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_sentence_transformer_model\u001b[39m(\n\u001b[1;32m     33\u001b[0m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     34\u001b[0m     token: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    Checks if the given model name or path corresponds to a SentenceTransformer model.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        bool: True if the model is a SentenceTransformer model, False otherwise.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\n\u001b[0;32m---> 53\u001b[0m         \u001b[43mload_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodules.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/sentence_transformers/util/file_io.py:96\u001b[0m, in \u001b[0;36mload_file_path\u001b[0;34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[1;32m     94\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(subfolder, filename)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1005\u001b[0m     )\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1070\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[0;32m-> 1070\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1543\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1543\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1547\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1548\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1460\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1457\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1460\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:283\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:306\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/lamoda-bootcamp-9UBAvNEw-py3.10/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:329\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n\u001b[1;32m    328\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms [Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_tries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m].\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 329\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Update sleep time for next retry\u001b[39;00m\n\u001b[1;32m    332\u001b[0m sleep_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_wait_time, sleep_time \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROJECT SETUP & MODEL INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "PROJECT_DIR = Path().resolve()\n",
    "INTERIM_DIR = PROJECT_DIR / \"data/interim\"\n",
    "FILE_PATH = PROJECT_DIR / \"data/raw/lamoda_reviews.csv\"\n",
    "\n",
    "# Проверяем, какие модели есть в кеше\n",
    "import os\n",
    "from pathlib import Path as PathLib\n",
    "\n",
    "cache_dir = PathLib.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
    "print(\"Checking local cache for models...\")\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# Список моделей для проверки\n",
    "model_candidates = [\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-mpnet-base-v2', \n",
    "    'paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'distilbert-base-nli-mean-tokens',\n",
    "    'paraphrase-MiniLM-L6-v2',\n",
    "]\n",
    "\n",
    "sentence_model = None\n",
    "loaded_model_name = None\n",
    "USE_TFIDF_FALLBACK = False\n",
    "\n",
    "# Пробуем загрузить из кеша\n",
    "for model_name in model_candidates:\n",
    "    try:\n",
    "        print(f\"\\nTrying model: {model_name}...\")\n",
    "        sentence_model = SentenceTransformer(model_name, local_files_only=True)\n",
    "        loaded_model_name = model_name\n",
    "        print(f\"✓ Model '{model_name}' loaded successfully from cache!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {model_name}: Not found in cache\")\n",
    "        continue\n",
    "\n",
    "# Если ничего не найдено, используем TF-IDF как fallback\n",
    "if sentence_model is None:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"⚠ No sentence transformer models found in cache\")\n",
    "    print(\"Using TF-IDF as fallback for embeddings\")\n",
    "    print(\"=\"*80)\n",
    "    USE_TFIDF_FALLBACK = True\n",
    "else:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"✓ Using model: {loaded_model_name}\")\n",
    "    print(f\"  Embedding dimension: {sentence_model.get_sentence_embedding_dimension()}\")\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f0872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "print(f\"Loaded {len(df)} reviews\")\n",
    "print(f\"Unique products: {df['product_sku'].nunique()}\")\n",
    "print(f\"Categories: {df['good_type'].unique()}\")\n",
    "print(f\"Subtypes: {df['good_subtype'].nunique()} unique subtypes\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb1095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "comment_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "product_sku",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "comment_text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "good_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "good_subtype",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0f5d7e3c-6c98-4c8d-8ac0-88899f3c21c4",
       "rows": [
        [
         "0",
         "395865747",
         "MP002XB078CD",
         "Купили сыну на день рождения!Теплая, но не жаркая. Оптимальная длина. Подросток 13 лет доволен весьма.",
         "Куртка утепленная",
         "Clothes",
         "OUTWEAR"
        ],
        [
         "1",
         "436891792",
         "MP002XB07Z8I",
         "Приятная вещь, симпатично смотрится ",
         "Свитшот",
         "Clothes",
         "SWEATSHIRTS"
        ],
        [
         "2",
         "383386833",
         "MP002XC00LSY",
         "Произведено в Турции. Качество хорошее и размеру соответствует.",
         "Поло",
         "Clothes",
         "TEE-SHIRTS & POLOS"
        ],
        [
         "3",
         "400670943",
         "MP002XC01NLE",
         "Классный комплект",
         "Боди и ползунки",
         "Clothes",
         "TEE-SHIRTS & POLOS"
        ],
        [
         "4",
         "388822372",
         "MP002XG03J2N",
         "Отличные перчатки, плотные, яркие. Ребенку 8 лет подошли хорошо. Цена вообще скпер-пупер",
         "Перчатки",
         "Accs",
         "GLOVES & MITTENS"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>product_sku</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>name</th>\n",
       "      <th>good_type</th>\n",
       "      <th>good_subtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>395865747</td>\n",
       "      <td>MP002XB078CD</td>\n",
       "      <td>Купили сыну на день рождения!Теплая, но не жар...</td>\n",
       "      <td>Куртка утепленная</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>OUTWEAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>436891792</td>\n",
       "      <td>MP002XB07Z8I</td>\n",
       "      <td>Приятная вещь, симпатично смотрится</td>\n",
       "      <td>Свитшот</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>SWEATSHIRTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383386833</td>\n",
       "      <td>MP002XC00LSY</td>\n",
       "      <td>Произведено в Турции. Качество хорошее и разме...</td>\n",
       "      <td>Поло</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>TEE-SHIRTS &amp; POLOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400670943</td>\n",
       "      <td>MP002XC01NLE</td>\n",
       "      <td>Классный комплект</td>\n",
       "      <td>Боди и ползунки</td>\n",
       "      <td>Clothes</td>\n",
       "      <td>TEE-SHIRTS &amp; POLOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>388822372</td>\n",
       "      <td>MP002XG03J2N</td>\n",
       "      <td>Отличные перчатки, плотные, яркие. Ребенку 8 л...</td>\n",
       "      <td>Перчатки</td>\n",
       "      <td>Accs</td>\n",
       "      <td>GLOVES &amp; MITTENS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   comment_id   product_sku  \\\n",
       "0   395865747  MP002XB078CD   \n",
       "1   436891792  MP002XB07Z8I   \n",
       "2   383386833  MP002XC00LSY   \n",
       "3   400670943  MP002XC01NLE   \n",
       "4   388822372  MP002XG03J2N   \n",
       "\n",
       "                                        comment_text               name  \\\n",
       "0  Купили сыну на день рождения!Теплая, но не жар...  Куртка утепленная   \n",
       "1               Приятная вещь, симпатично смотрится             Свитшот   \n",
       "2  Произведено в Турции. Качество хорошее и разме...               Поло   \n",
       "3                                  Классный комплект    Боди и ползунки   \n",
       "4  Отличные перчатки, плотные, яркие. Ребенку 8 л...           Перчатки   \n",
       "\n",
       "  good_type        good_subtype  \n",
       "0   Clothes             OUTWEAR  \n",
       "1   Clothes         SWEATSHIRTS  \n",
       "2   Clothes  TEE-SHIRTS & POLOS  \n",
       "3   Clothes  TEE-SHIRTS & POLOS  \n",
       "4      Accs    GLOVES & MITTENS  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# (1) TEXT CLEANING - МЯГКИЕ ПРАВИЛА\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text_soft(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Мягкая очистка текста: минимальная обработка\n",
    "    - Удаляем множественные пробелы\n",
    "    - Сохраняем все знаки препинания и эмодзи\n",
    "    - Приводим к нижнему регистру\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Удаляем множественные пробелы и переносы строк\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Удаляем только управляющие символы, сохраняем все остальное\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "# Применяем очистку\n",
    "print(\"Cleaning texts...\")\n",
    "df['clean_text'] = df['comment_text'].apply(clean_text_soft)\n",
    "df['clean_text'] = df['clean_text'].fillna('')\n",
    "\n",
    "# Удаляем пустые отзывы\n",
    "df = df[df['clean_text'].str.len() > 10].copy()\n",
    "print(f\"After cleaning: {len(df)} reviews\")\n",
    "df[['comment_text', 'clean_text', 'good_type', 'good_subtype']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# (2) EMBEDDINGS - ОДНИ И ТЕ ЖЕ ДЛЯ ВСЕХ ОТЗЫВОВ\n",
    "# ============================================================================\n",
    "\n",
    "def get_embeddings(texts: List[str], batch_size: int = 32) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Получение embeddings для списка текстов\n",
    "    Используется одна и та же модель для всех отзывов\n",
    "    Если sentence transformer недоступен, используем TF-IDF\n",
    "    \"\"\"\n",
    "    if USE_TFIDF_FALLBACK:\n",
    "        print(f\"Generating TF-IDF embeddings for {len(texts)} texts...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=384,  # Примерно как у MiniLM-L6-v2\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=1,\n",
    "            max_df=0.95\n",
    "        )\n",
    "        embeddings = vectorizer.fit_transform(texts).toarray()\n",
    "        print(f\"✓ Generated TF-IDF embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    else:\n",
    "        if sentence_model is None:\n",
    "            raise ValueError(\"Sentence transformer model is not loaded!\")\n",
    "        \n",
    "        print(f\"Generating sentence transformer embeddings for {len(texts)} texts...\")\n",
    "        embeddings = sentence_model.encode(\n",
    "            texts,\n",
    "            batch_size=batch_size,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "        print(f\"✓ Generated embeddings shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Генерируем embeddings для всех отзывов\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 2: GENERATING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Для тестирования можно использовать подмножество\n",
    "USE_SUBSET = True\n",
    "if USE_SUBSET:\n",
    "    df_sample = df.head(10000).copy()  # Используем 10k для тестирования\n",
    "    print(f\"Using subset: {len(df_sample)} reviews\")\n",
    "else:\n",
    "    df_sample = df.copy()\n",
    "\n",
    "# Получаем embeddings\n",
    "review_texts = df_sample['clean_text'].tolist()\n",
    "embeddings = get_embeddings(review_texts)\n",
    "\n",
    "# Сохраняем embeddings в DataFrame\n",
    "df_sample['embedding'] = list(embeddings)\n",
    "print(f\"✓ Embeddings added to dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb6cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# (3) CLUSTERING - ОДНА ЛОГИКА КЛАСТЕРИЗАЦИИ\n",
    "# ============================================================================\n",
    "\n",
    "def cluster_embeddings(\n",
    "    embeddings: np.ndarray,\n",
    "    n_clusters: Optional[int] = None,\n",
    "    min_clusters: int = 3,\n",
    "    max_clusters: int = 10\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Кластеризация embeddings с автоматическим выбором числа кластеров\n",
    "    Используется одна и та же логика для всех продуктов\n",
    "    \"\"\"\n",
    "    n_samples = len(embeddings)\n",
    "    \n",
    "    # Автоматический выбор числа кластеров\n",
    "    if n_clusters is None:\n",
    "        # Используем правило локтя или фиксированный диапазон\n",
    "        if n_samples < 10:\n",
    "            n_clusters = min(3, n_samples)\n",
    "        elif n_samples < 50:\n",
    "            n_clusters = min(5, n_samples // 2)\n",
    "        else:\n",
    "            # Пробуем несколько значений и выбираем лучшее по silhouette score\n",
    "            best_score = -1\n",
    "            best_k = min_clusters\n",
    "            \n",
    "            for k in range(min_clusters, min(max_clusters + 1, n_samples // 2)):\n",
    "                try:\n",
    "                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                    labels = kmeans.fit_predict(embeddings)\n",
    "                    score = silhouette_score(embeddings, labels)\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best_k = k\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            n_clusters = best_k\n",
    "    \n",
    "    # Выполняем кластеризацию\n",
    "    print(f\"Clustering {n_samples} embeddings into {n_clusters} clusters...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    print(f\"✓ Clustering completed. Cluster distribution:\")\n",
    "    unique, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for cluster_id, count in zip(unique, counts):\n",
    "        print(f\"  Cluster {cluster_id}: {count} reviews\")\n",
    "    \n",
    "    return cluster_labels, n_clusters\n",
    "\n",
    "\n",
    "# Применяем кластеризацию на уровне продукта\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 3: CLUSTERING EMBEDDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Группируем по продуктам и кластеризуем отзывы каждого продукта\n",
    "results = []\n",
    "\n",
    "for sku, group in df_sample.groupby('product_sku'):\n",
    "    if len(group) < 3:  # Минимум 3 отзыва для кластеризации\n",
    "        continue\n",
    "    \n",
    "    # Получаем embeddings для отзывов этого продукта\n",
    "    product_embeddings = np.array(group['embedding'].tolist())\n",
    "    \n",
    "    # Кластеризуем\n",
    "    cluster_labels, n_clusters = cluster_embeddings(product_embeddings)\n",
    "    \n",
    "    # Добавляем метки кластеров\n",
    "    group = group.copy()\n",
    "    group['cluster'] = cluster_labels\n",
    "    \n",
    "    results.append(group)\n",
    "\n",
    "# Объединяем результаты\n",
    "df_clustered = pd.concat(results, ignore_index=True)\n",
    "print(f\"\\n✓ Clustered {len(df_clustered)} reviews across {df_clustered['product_sku'].nunique()} products\")\n",
    "df_clustered[['product_sku', 'clean_text', 'cluster', 'good_type', 'good_subtype']].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f25161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# (4) ASPECT EXTRACTION - CATEGORY-AWARE\n",
    "# ============================================================================\n",
    "\n",
    "# Словари аспектов для разных категорий товаров\n",
    "CATEGORY_ASPECTS = {\n",
    "    'Clothes': {\n",
    "        'размер': ['размер', 'размеру', 'размера', 'размеры', 'подошёл', 'подошла', 'подошли', 'маленький', 'большой'],\n",
    "        'материал': ['материал', 'ткань', 'хлопок', 'синтетика', 'шерсть', 'полиэстер'],\n",
    "        'качество': ['качество', 'качественный', 'качественная', 'качественные', 'хороший', 'плохой'],\n",
    "        'цвет': ['цвет', 'цвета', 'окраска', 'яркий', 'яркая', 'яркие', 'красивый'],\n",
    "        'комфорт': ['удобный', 'удобная', 'удобные', 'комфортный', 'комфортная', 'комфортно'],\n",
    "        'стиль': ['стильный', 'стильная', 'стильные', 'модный', 'модная', 'модные'],\n",
    "        'цена': ['цена', 'стоимость', 'дешево', 'дорого', 'дорогой', 'дешевый']\n",
    "    },\n",
    "    'Shoes': {\n",
    "        'размер': ['размер', 'размеру', 'размера', 'размеры', 'подошёл', 'подошла', 'подошли'],\n",
    "        'комфорт': ['удобный', 'удобная', 'удобные', 'комфортный', 'комфортная', 'комфортно', 'натирает'],\n",
    "        'качество': ['качество', 'качественный', 'качественная', 'качественные', 'прочный', 'прочная'],\n",
    "        'материал': ['кожа', 'кожаный', 'кожаная', 'текстиль', 'резина'],\n",
    "        'внешний вид': ['красивый', 'красивая', 'красивые', 'симпатичный', 'стильный'],\n",
    "        'цена': ['цена', 'стоимость', 'дешево', 'дорого']\n",
    "    },\n",
    "    'Accs': {\n",
    "        'качество': ['качество', 'качественный', 'качественная', 'качественные'],\n",
    "        'материал': ['материал', 'кожа', 'металл', 'пластик'],\n",
    "        'внешний вид': ['красивый', 'красивая', 'красивые', 'симпатичный', 'стильный'],\n",
    "        'размер': ['размер', 'размеру', 'размера', 'размеры'],\n",
    "        'цена': ['цена', 'стоимость', 'дешево', 'дорого']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Общие аспекты для всех категорий\n",
    "COMMON_ASPECTS = {\n",
    "    'качество': ['качество', 'качественный', 'качественная', 'качественные', 'хороший', 'плохой'],\n",
    "    'цена': ['цена', 'стоимость', 'дешево', 'дорого', 'дорогой', 'дешевый'],\n",
    "    'внешний вид': ['красивый', 'красивая', 'красивые', 'симпатичный', 'симпатично', 'смотрится']\n",
    "}\n",
    "\n",
    "\n",
    "def extract_aspects_from_cluster(\n",
    "    cluster_texts: List[str],\n",
    "    cluster_embeddings: np.ndarray,\n",
    "    category: str,\n",
    "    good_subtype: Optional[str] = None\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Извлечение аспектов из кластера с учетом категории товара\n",
    "    \n",
    "    Args:\n",
    "        cluster_texts: Тексты отзывов в кластере\n",
    "        cluster_embeddings: Embeddings отзывов в кластере\n",
    "        category: Категория товара (Clothes, Shoes, Accs)\n",
    "        good_subtype: Подтип товара (опционально)\n",
    "    \n",
    "    Returns:\n",
    "        Словарь {aspect_name: representative_text}\n",
    "    \"\"\"\n",
    "    if len(cluster_texts) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Выбираем словарь аспектов для категории\n",
    "    aspect_keywords = CATEGORY_ASPECTS.get(category, COMMON_ASPECTS)\n",
    "    \n",
    "    # Находим наиболее репрезентативный отзыв в кластере (ближайший к центру)\n",
    "    center = cluster_embeddings.mean(axis=0)\n",
    "    distances = np.linalg.norm(cluster_embeddings - center, axis=1)\n",
    "    representative_idx = distances.argmin()\n",
    "    representative_text = cluster_texts[representative_idx]\n",
    "    \n",
    "    # Определяем, какие аспекты упоминаются в кластере\n",
    "    detected_aspects = {}\n",
    "    cluster_text_lower = ' '.join(cluster_texts).lower()\n",
    "    \n",
    "    for aspect_name, keywords in aspect_keywords.items():\n",
    "        # Проверяем наличие ключевых слов\n",
    "        if any(keyword in cluster_text_lower for keyword in keywords):\n",
    "            # Используем репрезентативный текст как значение аспекта\n",
    "            detected_aspects[aspect_name] = representative_text[:100]  # Ограничиваем длину\n",
    "    \n",
    "    return detected_aspects\n",
    "\n",
    "\n",
    "def extract_aspects_for_product(\n",
    "    product_df: pd.DataFrame,\n",
    "    category: str,\n",
    "    good_subtype: Optional[str] = None\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Извлечение аспектов для продукта на основе кластеров\n",
    "    \n",
    "    Returns:\n",
    "        Словарь {aspect_name: [representative_texts]}\n",
    "    \"\"\"\n",
    "    product_aspects = {}\n",
    "    \n",
    "    # Обрабатываем каждый кластер отдельно\n",
    "    for cluster_id in product_df['cluster'].unique():\n",
    "        cluster_df = product_df[product_df['cluster'] == cluster_id]\n",
    "        cluster_texts = cluster_df['clean_text'].tolist()\n",
    "        cluster_embeddings = np.array(cluster_df['embedding'].tolist())\n",
    "        \n",
    "        # Извлекаем аспекты из кластера\n",
    "        cluster_aspects = extract_aspects_from_cluster(\n",
    "            cluster_texts,\n",
    "            cluster_embeddings,\n",
    "            category,\n",
    "            good_subtype\n",
    "        )\n",
    "        \n",
    "        # Объединяем аспекты\n",
    "        for aspect_name, text in cluster_aspects.items():\n",
    "            if aspect_name not in product_aspects:\n",
    "                product_aspects[aspect_name] = []\n",
    "            product_aspects[aspect_name].append(text)\n",
    "    \n",
    "    return product_aspects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d03124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применяем извлечение аспектов для всех продуктов\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 4: ASPECT EXTRACTION (CATEGORY-AWARE)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "product_results = []\n",
    "\n",
    "for sku, product_df in df_clustered.groupby('product_sku'):\n",
    "    category = product_df['good_type'].iloc[0] if 'good_type' in product_df.columns else None\n",
    "    good_subtype = product_df['good_subtype'].iloc[0] if 'good_subtype' in product_df.columns else None\n",
    "    product_name = product_df['name'].iloc[0] if 'name' in product_df.columns else None\n",
    "    \n",
    "    # Извлекаем аспекты\n",
    "    aspects = extract_aspects_for_product(product_df, category, good_subtype)\n",
    "    \n",
    "    # Формируем теги из аспектов (берем по одному репрезентативному тексту на аспект)\n",
    "    tags = []\n",
    "    for aspect_name, texts in aspects.items():\n",
    "        if texts:\n",
    "            # Берем первый (наиболее репрезентативный) текст\n",
    "            tag = texts[0]\n",
    "            # Укорачиваем если нужно\n",
    "            if len(tag) > 80:\n",
    "                tag = tag[:77] + \"...\"\n",
    "            tags.append(tag)\n",
    "    \n",
    "    product_results.append({\n",
    "        'product_sku': sku,\n",
    "        'product_name': product_name,\n",
    "        'category': category,\n",
    "        'good_subtype': good_subtype,\n",
    "        'num_reviews': len(product_df),\n",
    "        'num_clusters': product_df['cluster'].nunique(),\n",
    "        'aspects': aspects,\n",
    "        'tags': tags[:6],  # Максимум 6 тегов\n",
    "        'tags_count': min(len(tags), 6)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(product_results)\n",
    "print(f\"\\n✓ Extracted aspects for {len(results_df)} products\")\n",
    "print(f\"Average tags per product: {results_df['tags_count'].mean():.2f}\")\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead0266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXAMINE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RESULTS EXAMINATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Показываем примеры для разных категорий\n",
    "for category in results_df['category'].unique()[:3]:\n",
    "    if pd.isna(category):\n",
    "        continue\n",
    "    \n",
    "    cat_products = results_df[results_df['category'] == category].head(3)\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Category: {category}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for _, row in cat_products.iterrows():\n",
    "        print(f\"\\nProduct: {row['product_name']} (SKU: {row['product_sku']})\")\n",
    "        print(f\"Reviews: {row['num_reviews']}, Clusters: {row['num_clusters']}\")\n",
    "        print(f\"Tags ({row['tags_count']}):\")\n",
    "        for i, tag in enumerate(row['tags'], 1):\n",
    "            print(f\"  {i}. {tag}\")\n",
    "        \n",
    "        # Показываем обнаруженные аспекты\n",
    "        if row['aspects']:\n",
    "            print(f\"\\nDetected aspects:\")\n",
    "            for aspect_name, texts in row['aspects'].items():\n",
    "                print(f\"  - {aspect_name}: {len(texts)} mentions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "# Сохраняем результаты\n",
    "output_path = INTERIM_DIR / \"product_aspects.csv\"\n",
    "\n",
    "# Подготавливаем данные для сохранения\n",
    "export_df = results_df.copy()\n",
    "export_df['tags'] = export_df['tags'].apply(lambda x: ' | '.join(x) if isinstance(x, list) else '')\n",
    "export_df['aspects'] = export_df['aspects'].apply(lambda x: json.dumps(x, ensure_ascii=False) if isinstance(x, dict) else '{}')\n",
    "\n",
    "export_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"✓ Results saved to: {output_path}\")\n",
    "\n",
    "# Также сохраняем в JSON\n",
    "output_json_path = INTERIM_DIR / \"product_aspects.json\"\n",
    "results_df.to_json(output_json_path, orient='records', force_ascii=False, indent=2)\n",
    "print(f\"✓ Results also saved to: {output_json_path}\")\n",
    "\n",
    "# Статистика\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total products processed: {len(results_df)}\")\n",
    "print(f\"Products with tags: {(results_df['tags_count'] > 0).sum()}\")\n",
    "print(f\"Average tags per product: {results_df['tags_count'].mean():.2f}\")\n",
    "print(f\"Average clusters per product: {results_df['num_clusters'].mean():.2f}\")\n",
    "print(f\"\\nTags distribution:\")\n",
    "print(results_df['tags_count'].value_counts().sort_index())\n",
    "print(f\"\\nCategories distribution:\")\n",
    "print(results_df['category'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72451d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PIPELINE SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PIPELINE SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Pipeline structure:\n",
    "  1. Отзывы + category + product_type\n",
    "     ↓\n",
    "  2. Очистка (мягкие правила) - минимальная обработка текста\n",
    "     ↓\n",
    "  3. Embeddings (одни и те же) - единая модель для всех отзывов\n",
    "     ↓\n",
    "  4. Кластеризация (одна логика) - единая логика кластеризации\n",
    "     ↓\n",
    "  5. Aspect extraction ← category-aware - извлечение с учетом категории\n",
    "\n",
    "Key features:\n",
    "  - Мягкая очистка: сохраняем максимум информации\n",
    "  - Единые embeddings: одна модель для всех отзывов\n",
    "  - Автоматическая кластеризация: выбор оптимального числа кластеров\n",
    "  - Category-aware aspects: разные словари аспектов для разных категорий\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b98ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PROCESS FULL DATASET (OR SAMPLE)\n",
    "# ============================================================================\n",
    "\n",
    "# Обрабатываем датасет\n",
    "# Для быстрого тестирования используем sample_size\n",
    "# Для полной обработки установите sample_size=None\n",
    "\n",
    "SAMPLE_SIZE = 50  # Обработать топ-50 товаров по количеству отзывов\n",
    "# SAMPLE_SIZE = None  # Раскомментируйте для обработки всех товаров\n",
    "\n",
    "print(\"Starting batch processing...\")\n",
    "results_df = process_dataset(df, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "print(f\"\\nProcessed {len(results_df)} products\")\n",
    "print(f\"Average tags per product: {results_df['tags_count'].mean():.2f}\")\n",
    "print(f\"\\nResults preview:\")\n",
    "results_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8586354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPORT RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Сохраняем результаты\n",
    "output_path = INTERIM_DIR / \"product_tags.csv\"\n",
    "results_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Также сохраняем в более читаемом формате (JSON)\n",
    "import json\n",
    "output_json_path = INTERIM_DIR / \"product_tags.json\"\n",
    "\n",
    "# Конвертируем список тегов в строку для JSON\n",
    "json_data = results_df.copy()\n",
    "json_data['tags'] = json_data['tags'].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "\n",
    "json_data.to_json(output_json_path, orient='records', force_ascii=False, indent=2)\n",
    "print(f\"Results also saved to: {output_json_path}\")\n",
    "\n",
    "# Показываем статистику\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Total products processed: {len(results_df)}\")\n",
    "print(f\"Products with tags: {(results_df['tags_count'] > 0).sum()}\")\n",
    "print(f\"Average tags per product: {results_df['tags_count'].mean():.2f}\")\n",
    "print(f\"Min tags: {results_df['tags_count'].min()}\")\n",
    "print(f\"Max tags: {results_df['tags_count'].max()}\")\n",
    "print(f\"\\nTags distribution:\")\n",
    "print(results_df['tags_count'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION: EXAMINE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "# Показываем примеры результатов для разных категорий\n",
    "print(\"Examples by category:\\n\")\n",
    "\n",
    "for category in results_df['category'].unique()[:3]:\n",
    "    if pd.isna(category):\n",
    "        continue\n",
    "    cat_products = results_df[results_df['category'] == category].head(3)\n",
    "    print(f\"\\n{category}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for _, row in cat_products.iterrows():\n",
    "        print(f\"\\n  SKU: {row['product_sku']}\")\n",
    "        print(f\"  Name: {row['product_name']}\")\n",
    "        print(f\"  Reviews: {row['num_reviews']}\")\n",
    "        print(f\"  Tags ({row['tags_count']}):\")\n",
    "        for tag in row['tags']:\n",
    "            print(f\"    - {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3c7c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1774267, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49e351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamoda-bootcamp-9UBAvNEw-py3.10 (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
