<<<<<<< Current (Your changes)
# Lamoda Bootcamp

ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¾Ñ‚Ð·Ñ‹Ð²Ð¾Ð² Lamoda Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ LLM.
=======
# Lamoda Review Tag Inference System

Production-ready LLM-based system for analyzing product reviews and extracting relevant tags using OpenAI API.

## ðŸ—ï¸ Project Structure

```
oez/
â”œâ”€â”€ src/                          # Main source code
â”‚   â”œâ”€â”€ clients/                  # LLM client implementations
â”‚   â”‚   â”œâ”€â”€ llm_client.py        # Protocol interface for LLM clients
â”‚   â”‚   â””â”€â”€ openai_client.py     # OpenAI API client implementation
â”‚   â”œâ”€â”€ core/                     # Core business logic
â”‚   â”‚   â”œâ”€â”€ tag_inference.py     # Main inference pipeline
â”‚   â”‚   â””â”€â”€ pipeline.py          # Batch processing pipelines
â”‚   â”œâ”€â”€ utils/                    # Utility functions
â”‚   â”‚   â”œâ”€â”€ data.py              # Data loading utilities
â”‚   â”‚   â”œâ”€â”€ preprocessing.py     # Review preprocessing
â”‚   â”‚   â”œâ”€â”€ postprocessing.py    # Tag postprocessing
â”‚   â”‚   â”œâ”€â”€ prompt_builder.py    # LLM prompt construction
â”‚   â”‚   â””â”€â”€ llm_executor.py      # LLM execution with retries
â”‚   â””â”€â”€ config.py                 # Configuration management
â”œâ”€â”€ examples/                     # Usage examples
â”‚   â”œâ”€â”€ mock_client.py           # Mock LLM client for testing
â”‚   â”œâ”€â”€ example_basic_inference.py
â”‚   â””â”€â”€ example_pipeline.py
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for experiments
â”œâ”€â”€ data/                         # Data directory
â”‚   â”œâ”€â”€ raw/                     # Raw data files
â”‚   â””â”€â”€ processed/               # Processed results
â”œâ”€â”€ .env.example                  # Environment variables template
â”œâ”€â”€ run_pipeline_openai.py       # Main script for running pipeline
â””â”€â”€ pyproject.toml               # Project dependencies

```

## ðŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd oez

# Install dependencies using Poetry
poetry install

# Or using pip
pip install -r requirements.txt
```

### 2. Configuration

Create a `.env` file from the template:

```bash
cp .env.example .env
```

Edit `.env` and set your OpenAI API key:

```env
OPENAI_API_KEY=your_actual_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.3
OPENAI_MAX_TOKENS=200

# Pipeline Configuration
MAX_REVIEWS_PER_SKU=50
MAX_TAGS_PER_SKU=6
MAX_CHARS_PER_REVIEW=500
MIN_REVIEW_LENGTH=10
MIN_REVIEWS_PER_SKU=1

# Data Paths
INPUT_CSV_PATH=data/raw/lamoda_reviews.csv
OUTPUT_CSV_PATH=data/processed/llm_tags_results.csv

# Logging
LOG_LEVEL=INFO
```

### 3. Running the Pipeline

#### Basic Usage

```bash
# Run with default settings from .env
python run_pipeline_openai.py
```

#### Advanced Usage

```bash
# Process limited number of SKUs for testing
python run_pipeline_openai.py --limit 10

# Override input/output paths
python run_pipeline_openai.py --input data.csv --output results.csv

# Use different OpenAI model
python run_pipeline_openai.py --model gpt-4

# Adjust verbosity
python run_pipeline_openai.py --log-level DEBUG
```

## ðŸ“š Usage Examples

### Basic Inference

```python
from src import OpenAIClient, run_inference

# Initialize client
client = OpenAIClient()  # Reads API key from .env

# Prepare data
reviews = ["Great quality!", "Perfect size"]
name_to_tags = {"T-Shirt": ["quality", "size", "price"]}

# Run inference
tags = run_inference(
    reviews=reviews,
    llm_client=client,
    name_to_tags=name_to_tags,
    subtype_to_tags={},
    type_to_tags={},
    product_name="T-Shirt"
)

print(f"Extracted tags: {tags}")
```

### Batch Processing

```python
from src import OpenAIClient, run_pipeline_for_file

# Initialize client
client = OpenAIClient()

# Prepare golden tags
golden_tags = {
    "T-Shirt": ["quality", "size", "material"],
    "Jeans": ["quality", "size", "fit"]
}

# Run pipeline for entire file
results = run_pipeline_for_file(
    csv_path="data/raw/reviews.csv",
    llm_client=client,
    name_to_tags=golden_tags,
    output_path="data/processed/results.csv",
    limit_skus=100  # Optional: limit for testing
)

print(f"Processed {len(results)} SKUs")
```

### Using Mock Client for Testing

```python
from examples.mock_client import MockLLMClient
from src import run_inference

# No API costs, instant responses
client = MockLLMClient()

tags = run_inference(
    reviews=["Great product"],
    llm_client=client,
    name_to_tags={"Product": ["quality", "price"]},
    subtype_to_tags={},
    type_to_tags={},
    product_name="Product"
)
```

## ðŸ­ Production Features

### Security & Configuration
- âœ… **Environment Variables**: All sensitive data (API keys) in `.env` file
- âœ… **Configuration Management**: Centralized config in `src/config.py`
- âœ… **Type Safety**: Full type hints for better IDE support and error detection

### Code Quality
- âœ… **Clean Architecture**: Separated concerns (clients, core, utils)
- âœ… **Protocol-based Design**: Easy to swap LLM providers
- âœ… **Logging**: Comprehensive logging throughout the codebase
- âœ… **Error Handling**: Retry logic and graceful error handling

### Maintainability
- âœ… **No Code Duplication**: Single source of truth for each functionality
- âœ… **Clear Naming**: Self-documenting code with descriptive names
- âœ… **Modular Design**: Easy to extend and test
- âœ… **Examples Separated**: Production code separate from examples

## ðŸ”§ Key Improvements from Original Code

### 1. **Removed Duplications**
- Merged `BaseLLMClient` and `LLMClient` into single Protocol
- Consolidated `MockLLMClient` into examples package
- Eliminated redundant functionality across modules

### 2. **Clearer File Names**
- `inference.py` â†’ `tag_inference.py` (more specific)
- `llm_inference.py` â†’ `llm_executor.py` (clearer purpose)
- `data_loader.py` â†’ `data.py` (cleaner name)
- `openai_client.py` â†’ `clients/openai_client.py` (better organization)

### 3. **Production-Ready Features**
- Environment variable management with `python-dotenv`
- Centralized configuration in `Config` class
- Comprehensive logging at all levels
- Proper error handling and retries
- Type hints throughout

### 4. **Security**
- No hardcoded API keys
- API keys loaded from environment variables
- `.env.example` template for easy setup
- `.env` in `.gitignore` (if not already)

## ðŸ“– API Reference

### Core Functions

#### `run_inference()`
Execute full inference cycle for a single product.

**Parameters:**
- `reviews`: List of review strings
- `llm_client`: LLM client implementing `LLMClient` protocol
- `name_to_tags`, `subtype_to_tags`, `type_to_tags`: Golden tags dictionaries
- `product_name`, `product_subtype`, `product_type`: Product metadata
- `max_chars`, `max_reviews`, `min_review_length`, `max_tags`: Processing parameters

**Returns:** List of extracted tags

#### `run_pipeline_for_file()`
Process entire CSV file with reviews.

**Parameters:**
- `csv_path`: Path to input CSV
- `llm_client`: LLM client
- Golden tags dictionaries
- `output_path`: Where to save results
- Processing parameters
- `limit_skus`: Optional limit for testing
- `skip_errors`: Continue on errors

**Returns:** pandas DataFrame with results

### Clients

#### `OpenAIClient`
Production client for OpenAI API.

```python
client = OpenAIClient(
    api_key="...",  # Optional, reads from OPENAI_API_KEY env var
    model="gpt-4o-mini",
    temperature=0.3,
    max_tokens=200
)
```

#### `MockLLMClient`
Test client that doesn't make real API calls.

```python
from examples.mock_client import MockLLMClient
client = MockLLMClient()
```

## ðŸ§ª Testing

Run the examples to verify your setup:

```bash
# Basic inference example
python -m examples.example_basic_inference

# Pipeline example (requires data file)
python -m examples.example_pipeline
```

## ðŸ“Š Expected Input Format

The CSV file should have these columns:
- `product_sku`: Unique product identifier
- `comment_text`: Review text
- `name`: Product name (optional)
- `good_subtype`: Product subtype (optional)
- `good_type`: Product type (optional)

## ðŸ“ TODO

- [ ] Add support for other LLM providers (Anthropic, Cohere, etc.)
- [ ] Add caching layer for LLM responses
- [ ] Add metrics and monitoring
- [ ] Add unit tests
- [ ] Add CI/CD pipeline

## ðŸ¤ Contributing

1. Create a feature branch
2. Make your changes
3. Ensure code follows the existing style
4. Update documentation if needed
5. Submit a pull request

## ðŸ“„ License

[Your License Here]

## ðŸ”— Links

- [OpenAI API Documentation](https://platform.openai.com/docs)
- [Python-dotenv Documentation](https://github.com/theskumar/python-dotenv)
>>>>>>> Incoming (Background Agent changes)
