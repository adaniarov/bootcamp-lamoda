# src/vector_pipeline.py
"""Pipeline –¥–ª—è vectorize-—Ä–µ–∂–∏–º–∞: –±–µ–∑ LLM, —Ç–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ GOLDEN_TAGS."""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import logging
import json

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer

from .data_loader import load_dataset, load_golden_tags_from_dict

logger = logging.getLogger(__name__)


# -----------------------------
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
# -----------------------------

@dataclass
class ProductMeta:
    sku: str
    name: Optional[str]
    subtype: Optional[str]
    type: Optional[str]
    reviews: List[str]
    num_reviews: int


@dataclass
class TagScore:
    tag: str
    score: float
    count: int


# -----------------------------
# –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤
# -----------------------------

class TagVectorizer:
    """
    –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —Ç–µ–≥–æ–≤ –∏ –æ—Ç–∑—ã–≤–æ–≤.

    –õ–æ–≥–∏–∫–∞:
      - —ç–º–±–µ–¥–¥–∏–º –≤—Å–µ GOLDEN_TAGS –æ–¥–∏–Ω —Ä–∞–∑;
      - –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç—ã:
        name -> tags, subtype -> tags, type -> tags;
      - –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤ –¥–µ–ª–∞–µ–º –æ–∫–Ω–∞ –ø–æ 4 —Å–ª–æ–≤–∞;
      - —Å—á–∏—Ç–∞–µ–º cosine_similarity –º–µ–∂–¥—É –æ–∫–Ω–æ–º –∏:
          * emb(TAG)
          * emb("–Ω–µ TAG")
        –∏ –≥–æ–ª–æ—Å—É–µ–º –∑–∞ —Ç–µ–≥–∏.
    """

    def __init__(
        self,
        model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        window_size: int = 4,
        window_step: int = 1,
        # üî• –±–æ–ª–µ–µ –∂–µ—Å—Ç–∫–∏–π –ø–æ—Ä–æ–≥ "–ø–æ—Ö–æ–∂–µ—Å—Ç–∏" –æ–∫–Ω–∞ –Ω–∞ —Ç–µ–≥
        sim_threshold: float = 0.75,
        # üî• —Ç—Ä–µ–±—É–µ–º –∑–∞–º–µ—Ç–Ω—ã–π —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É TAG –∏ "–Ω–µ TAG"
        neg_margin: float = 0.12,
        # üî• —á—É—Ç—å —Å—Ç—Ä–æ–∂–µ –¥–µ–¥—É–ø
        dedup_threshold: float = 0.8,
        max_tags: int = 6,
        # üî• –º–∏–Ω–∏–º—É–º –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –æ–∫–æ–Ω –¥–ª—è —Ç–µ–≥–∞
        min_tag_count: int = 3,
        min_review_length_words: int = 4,
        # üî• –º–∏–Ω–∏–º—É–º —Ä–∞–∑–Ω—ã—Ö –æ—Ç–∑—ã–≤–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç —Ç–µ–≥
        min_reviews_with_evidence: int = 2,
        # üî• –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π "—Å–∏–ª—å–Ω—ã–π" –º–∞–∫—Å–∏–º—É–º –ø–æ —Ç–µ–≥—É
        strong_max_threshold: float = 0.6,
        device: Optional[str] = None,
    ):
        self.model = SentenceTransformer(model_name, device=device)
        self.window_size = window_size
        self.window_step = window_step
        self.sim_threshold = sim_threshold
        self.neg_margin = neg_margin
        self.dedup_threshold = dedup_threshold
        self.max_tags = max_tags
        self.min_tag_count = min_tag_count
        self.min_review_length_words = min_review_length_words
        self.min_reviews_with_evidence = min_reviews_with_evidence
        self.strong_max_threshold = strong_max_threshold

        # cache: tag -> (tag_emb, neg_tag_emb)
        self._tag_embedding_cache: Dict[str, Tuple[np.ndarray, np.ndarray]] = {}

    # --------- —Å–ª—É–∂–µ–±–Ω—ã–µ –º–µ—Ç–æ–¥—ã ---------

    @staticmethod
    def _cosine_sim(a: np.ndarray, b: np.ndarray) -> np.ndarray:
        """
        a: (N, D), b: (D,)
        return: (N,)
        """
        a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-9)
        b_norm = b / (np.linalg.norm(b) + 1e-9)
        return np.dot(a_norm, b_norm)

    def _get_tag_embeddings(self, tag: str) -> Tuple[np.ndarray, np.ndarray]:
        if tag in self._tag_embedding_cache:
            return self._tag_embedding_cache[tag]

        texts = [tag, f"–Ω–µ {tag}"]
        embs = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=False)
        tag_emb, neg_emb = embs[0], embs[1]
        self._tag_embedding_cache[tag] = (tag_emb, neg_emb)
        return tag_emb, neg_emb

    @staticmethod
    def _tokenize(text: str) -> List[str]:
        # —Å—É–ø–µ—Ä –ø—Ä–æ—Å—Ç–æ–π —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä ‚Äî –ø–æ –ø—Ä–æ–±–µ–ª–∞–º, –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ —á—Ç–æ-—Ç–æ –ª—É—á—à–µ
        return [t for t in text.strip().split() if t]

    def _build_windows(self, review: str) -> List[str]:
        tokens = self._tokenize(review)
        if len(tokens) < self.min_review_length_words:
            return []

        windows: List[str] = []
        step = self.window_step
        size = self.window_size

        for i in range(0, max(1, len(tokens) - size + 1), step):
            window_tokens = tokens[i : i + size]
            if not window_tokens:
                continue
            windows.append(" ".join(window_tokens))

        # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –¥–æ–±–∞–≤–∏–º –≤–µ—Å—å –æ—Ç–∑—ã–≤
        if not windows:
            windows = [" ".join(tokens)]

        return windows

    # --------- –ø—É–±–ª–∏—á–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∞ ---------

    def infer_tags_for_product(
        self,
        reviews: List[str],
        candidate_tags: List[str],
        max_reviews: int = 50,
    ) -> List[str]:
        """
        –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–¥–Ω–æ–≥–æ SKU:
         - reviews: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –æ—Ç–∑—ã–≤–æ–≤,
         - candidate_tags: —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤-–∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ (–∏–∑ GOLDEN_TAGS),
         - max_reviews: –º–∞–∫—Å–∏–º—É–º –æ—Ç–∑—ã–≤–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –±–µ—Ä—ë–º (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏).

        ‚ö† –£—Å–ª–æ–≤–∏—è —Å—Ç–∞–ª–∏ —Å—Ç—Ä–æ–∂–µ:
          - —Ç–µ–≥ –¥–æ–ª–∂–µ–Ω –∏–º–µ—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –æ–∫–æ–Ω —Å –≤—ã—Å–æ–∫–æ–π –ø–æ—Ö–æ–∂–µ—Å—Ç—å—é,
          - —ç—Ç–∏ –æ–∫–Ω–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º N —Ä–∞–∑–Ω—ã–º –æ—Ç–∑—ã–≤–∞–º,
          - –º–∞–∫—Å–∏–º—É–º –ø–æ—Ö–æ–∂–µ—Å—Ç–∏ –ø–æ —Ç–µ–≥—É –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å >= strong_max_threshold,
          - "–Ω–µ TAG" –Ω–µ –¥–æ–ª–∂–µ–Ω –ø–æ–±–µ–∂–¥–∞—Ç—å.
        """
        if not reviews or not candidate_tags:
            return []

        # –æ–±—Ä–µ–∂–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        if max_reviews is not None and len(reviews) > max_reviews:
            reviews = reviews[:max_reviews]

        # —Å–æ–±–∏—Ä–∞–µ–º –æ–∫–Ω–∞ –ø–æ –≤—Å–µ–º –æ—Ç–∑—ã–≤–∞–º + –∏–Ω–¥–µ–∫—Å –æ—Ç–∑—ã–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –æ–∫–Ω–∞
        windows: List[str] = []
        window_review_idx: List[int] = []

        for review_idx, r in enumerate(reviews):
            r = str(r).strip()
            if not r:
                continue
            local_windows = self._build_windows(r)
            for w in local_windows:
                windows.append(w)
                window_review_idx.append(review_idx)

        if not windows:
            return []

        window_review_idx = np.array(window_review_idx, dtype=int)

        # —ç–º–±–µ–¥–¥–∏–º –≤—Å–µ –æ–∫–Ω–∞ –æ–¥–Ω–∏–º –±–∞—Ç—á–µ–º
        window_embs = self.model.encode(
            windows,
            convert_to_numpy=True,
            show_progress_bar=False,
        )  # shape: (N_windows, D)

        tag_scores: List[TagScore] = []

        for tag in candidate_tags:
            tag = tag.strip()
            if not tag:
                continue

            tag_emb, neg_emb = self._get_tag_embeddings(tag)

            # cosineSimilarity(window, TAG) & cosineSimilarity(window, "–Ω–µ TAG")
            sim_tag = self._cosine_sim(window_embs, tag_emb)  # (N,)
            sim_neg = self._cosine_sim(window_embs, neg_emb)  # (N,)

            max_sim_tag = sim_tag.max()
            max_sim_neg = sim_neg.max()

            # üî• 1) –µ—Å–ª–∏ –º–∞–∫—Å–∏–º—É–º –ø–æ "–Ω–µ TAG" –≤—ã—à–µ, —á–µ–º –ø–æ TAG + –∑–∞–ø–∞—Å ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º —Ç–µ–≥
            if max_sim_neg > max_sim_tag + self.neg_margin:
                continue

            # üî• 2) –µ—Å–ª–∏ —Å–∞–º –º–∞–∫—Å–∏–º—É–º –ø–æ TAG —Å–ª–∞–±–µ–µ, —á–µ–º strong_max_threshold ‚Äî –≤—ã–∫–∏–¥—ã–≤–∞–µ–º
            if max_sim_tag < self.strong_max_threshold:
                continue

            # –æ–∫–Ω–∞, –∫–æ—Ç–æ—Ä—ã–µ —Å—á–∏—Ç–∞–µ–º —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ–º –≤ –ø–æ–ª—å–∑—É —Ç–µ–≥–∞:
            # - sim_tag >= sim_threshold
            # - sim_tag > sim_neg + neg_margin
            mask_pos = (sim_tag >= self.sim_threshold) & (
                sim_tag > sim_neg + self.neg_margin
            )
            pos_indices = np.where(mask_pos)[0]

            if len(pos_indices) < self.min_tag_count:
                # üî• —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ —Å–∏–ª—å–Ω—ã—Ö –æ–∫–æ–Ω
                continue

            # üî• 3) –æ–∫–Ω–∞ –¥–æ–ª–∂–Ω—ã –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º N —Ä–∞–∑–Ω—ã–º –æ—Ç–∑—ã–≤–∞–º
            supported_reviews = np.unique(window_review_idx[pos_indices])
            if len(supported_reviews) < self.min_reviews_with_evidence:
                continue

            pos_scores = sim_tag[pos_indices]
            score = float(pos_scores.mean())
            count = int(len(pos_indices))
            tag_scores.append(TagScore(tag=tag, score=score, count=count))

        if not tag_scores:
            return []

        # —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–µ–≥–∏ –ø–æ score (—Å–Ω–∞—á–∞–ª–∞ —Å–∏–ª—å–Ω—ã–µ —Å–∏–≥–Ω–∞–ª—ã, –ø–æ—Ç–æ–º –ø–æ count)
        tag_scores.sort(key=lambda x: (x.score, x.count), reverse=True)

        # dedup –ø–æ—Ö–æ–∂–∏—Ö —Ç–µ–≥–æ–≤ –ø–æ cosine similarity –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å–∞–º–∏—Ö —Ç–µ–≥–æ–≤
        selected_tags: List[str] = []
        selected_embs: List[np.ndarray] = []

        for ts in tag_scores:
            emb_tag, _ = self._get_tag_embeddings(ts.tag)

            if not selected_embs:
                selected_tags.append(ts.tag)
                selected_embs.append(emb_tag)
                continue

            sims = [
                self._cosine_sim(emb_tag.reshape(1, -1), emb_prev)[0]
                for emb_prev in selected_embs
            ]
            if max(sims) >= self.dedup_threshold:
                # —Å–ª–∏—à–∫–æ–º –ø–æ—Ö–æ–∂ –Ω–∞ —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–µ–≥ ‚Äî —Å–∫–∏–ø–∞–µ–º
                continue

            selected_tags.append(ts.tag)
            selected_embs.append(emb_tag)

            if len(selected_tags) >= self.max_tags:
                break

        return selected_tags


# -----------------------------
# –£—Ç–∏–ª–∏—Ç–∞: –∑–∞–≥—Ä—É–∑–∫–∞ GOLDEN_TAGS
# -----------------------------

def load_golden_tags_from_json(
    golden_tags_path: Path,
) -> tuple[Dict[str, List[str]], Dict[str, List[str]], Dict[str, List[str]]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç golden_tags.json –∏ —Å—Ç—Ä–æ–∏—Ç —Ç—Ä–∏ —Å–ª–æ–≤–∞—Ä—è:
      name_to_tags, subtype_to_tags, type_to_tags.
    –§–æ—Ä–º–∞—Ç –æ–∂–∏–¥–∞–µ—Ç—Å—è –ø—Ä–∏–º–µ—Ä–Ω–æ —Ç–∞–∫–æ–π:
      [
        {"name": "–§—É—Ç–±–æ–ª–∫–∞", "tags": ["–º—è–≥–∫–∏–π —Ö–ª–æ–ø–æ–∫", ...]},
        {"subtype": "–ö—Ä–æ—Å—Å–æ–≤–∫–∏ —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–µ", "tags": [...]},
        {"type": "–û–±—É–≤—å", "tags": [...]},
        ...
      ]
    """
    with open(golden_tags_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    name_to_tags: Dict[str, List[str]] = {}
    subtype_to_tags: Dict[str, List[str]] = {}
    type_to_tags: Dict[str, List[str]] = {}

    for item in data:
        tags = item.get("tags") or []
        if isinstance(tags, str):
            tags = [t.strip() for t in tags.split(",") if t.strip()]

        if not isinstance(tags, list):
            continue

        name = item.get("name")
        subtype = item.get("subtype") or item.get("good_subtype")
        product_type = item.get("type") or item.get("good_type")

        def add_to_dict(d: Dict[str, List[str]], key: Optional[str]):
            if not key:
                return
            if key not in d:
                d[key] = []
            d[key].extend(tags)

        add_to_dict(name_to_tags, name)
        add_to_dict(subtype_to_tags, subtype)
        add_to_dict(type_to_tags, product_type)

    # —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –≤–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ø–∏—Å–∫–∞
    name_to_tags = {k: sorted(set(v)) for k, v in name_to_tags.items()}
    subtype_to_tags = {k: sorted(set(v)) for k, v in subtype_to_tags.items()}
    type_to_tags = {k: sorted(set(v)) for k, v in type_to_tags.items()}

    return name_to_tags, subtype_to_tags, type_to_tags


def get_candidate_tags_for_product(
    name: Optional[str],
    subtype: Optional[str],
    product_type: Optional[str],
    name_tags: Dict[str, List[str]],
    subtype_tags: Dict[str, List[str]],
    type_tags: Dict[str, List[str]],
) -> List[str]:
    """
    –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–µ–≥–æ–≤ –ø–æ –ø—Ä–∞–≤–∏–ª—É:
      - —Å–Ω–∞—á–∞–ª–∞ –ø–æ name,
      - –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ name, —Ç–æ –ø–æ subtype,
      - –µ—Å–ª–∏ –Ω–µ—Ç –ø–æ subtype, —Ç–æ –ø–æ type.
    """
    candidates: List[str] = []

    if name and name in name_tags:
        candidates.extend(name_tags[name])

    if not candidates and subtype and subtype in subtype_tags:
        candidates.extend(subtype_tags[subtype])

    if not candidates and product_type and product_type in type_tags:
        candidates.extend(type_tags[product_type])

    # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    candidates = sorted(set(t.strip() for t in candidates if t.strip()))
    return candidates


# -----------------------------
# Pipeline –¥–ª—è —Ñ–∞–π–ª–∞ (vectorize)
# -----------------------------

def run_vector_pipeline_for_file(
    csv_path: str,
    golden_tags_path: str,
    output_path: Optional[str] = None,
    max_chars: int = 500,
    max_reviews: int = 50,
    min_review_length: int = 10,
    max_tags: int = 6,
    min_reviews_per_sku: int = 1,
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    device: Optional[str] = None,
) -> pd.DataFrame:
    """
    –ü–æ–ª–Ω—ã–π vectorize-pipeline:
      1. –ó–∞–≥—Ä—É–∂–∞–µ–º CSV —á–µ—Ä–µ–∑ load_dataset (–ø–æ sku).
      2. –ó–∞–≥—Ä—É–∂–∞–µ–º GOLDEN_TAGS –∏ —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä–∏ name/subtype/type -> tags.
      3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ SKU:
         - —Å–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–µ–≥–æ–≤,
         - –ø—Ä–æ–≥–æ–Ω—è–µ–º —á–µ—Ä–µ–∑ TagVectorizer,
         - —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.
    """
    logger.info(f"[VECTOR] –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {csv_path}")
    csv_path_obj = Path(csv_path)
    golden_tags_path_obj = Path(golden_tags_path)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ SKU
    sku_data_raw = load_dataset(
        csv_path=str(csv_path_obj),
        min_reviews_per_sku=min_reviews_per_sku,
    )
    logger.info(f"[VECTOR] –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sku_data_raw)} SKU —Å –æ—Ç–∑—ã–≤–∞–º–∏")

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º GOLDEN_TAGS
    name_tags, subtype_tags, type_tags = load_golden_tags_from_json(golden_tags_path_obj)
    logger.info(
        f"[VECTOR] GOLDEN_TAGS: name={len(name_tags)}, "
        f"subtype={len(subtype_tags)}, type={len(type_tags)}"
    )

    # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º TagVectorizer
    vectorizer = TagVectorizer(
        model_name=model_name,
        device=device,
        max_tags=max_tags,
    )

    # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö SKU
    results: List[Dict[str, Any]] = []

    for i, (sku, pdata) in enumerate(sku_data_raw.items(), start=1):
        product = ProductMeta(
            sku=sku,
            name=pdata.get("name"),
            subtype=pdata.get("subtype"),
            type=pdata.get("type"),
            reviews=pdata.get("reviews", []),
            num_reviews=pdata.get("num_reviews", 0),
        )

        logger.info(
            f"[VECTOR] {i}/{len(sku_data_raw)} SKU={product.sku}, "
            f"name={product.name}, reviews={product.num_reviews}"
        )

        # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –¥–ª–∏–Ω–µ –∏ —Å–∏–º–≤–æ–ª–∞–º
        filtered_reviews = []
        for r in product.reviews:
            r = str(r).strip()
            if len(r) < min_review_length:
                continue
            if len(r) > max_chars:
                r = r[:max_chars]
            filtered_reviews.append(r)

        if not filtered_reviews:
            results.append(
                {
                    "sku": product.sku,
                    "name": product.name,
                    "subtype": product.subtype,
                    "type": product.type,
                    "tags": "",
                    "num_tags": 0,
                    "num_reviews": product.num_reviews,
                    "error": "no_valid_reviews",
                }
            )
            continue

        # –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–µ–≥–æ–≤
        candidate_tags = get_candidate_tags_for_product(
            name=product.name,
            subtype=product.subtype,
            product_type=product.type,
            name_tags=name_tags,
            subtype_tags=subtype_tags,
            type_tags=type_tags,
        )

        if not candidate_tags:
            results.append(
                {
                    "sku": product.sku,
                    "name": product.name,
                    "subtype": product.subtype,
                    "type": product.type,
                    "tags": "",
                    "num_tags": 0,
                    "num_reviews": product.num_reviews,
                    "error": "no_candidate_tags",
                }
            )
            continue

        # –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–µ–≥–æ–≤
        tags = vectorizer.infer_tags_for_product(
            reviews=filtered_reviews,
            candidate_tags=candidate_tags,
            max_reviews=max_reviews,
        )

        results.append(
            {
                "sku": product.sku,
                "name": product.name,
                "subtype": product.subtype,
                "type": product.type,
                "tags": ", ".join(tags),
                "num_tags": len(tags),
                "num_reviews": product.num_reviews,
                "error": None if tags else "no_selected_tags",
            }
        )

    df_results = pd.DataFrame(results)

    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if output_path:
        out_path = Path(output_path)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        df_results.to_csv(out_path, index=False, encoding="utf-8")
        logger.info(f"[VECTOR] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {out_path}")

    logger.info(
        f"[VECTOR] –ì–æ—Ç–æ–≤–æ: {len(df_results)} SKU, "
        f"{len(df_results[df_results['num_tags'] > 0])} —Å —Ç–µ–≥–∞–º–∏"
    )

    return df_results

def run_vector_pipeline_for_sku(
    csv_path: str,
    golden_tags_path: str,
    sku: str,
    max_chars: int = 500,
    max_reviews: int = 50,
    min_review_length: int = 10,
    max_tags: int = 6,
    min_reviews_per_sku: int = 1,
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    device: Optional[str] = None,
) -> List[str]:
    """
    Vectorize-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–¥–Ω–æ–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π SKU.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤ (<= max_tags).
    """
    csv_path_obj = Path(csv_path)
    golden_tags_path_obj = Path(golden_tags_path)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ SKU
    sku_data_raw = load_dataset(
        csv_path=str(csv_path_obj),
        min_reviews_per_sku=min_reviews_per_sku,
    )

    if sku not in sku_data_raw:
        raise ValueError(f"SKU '{sku}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Ñ–∞–π–ª–µ {csv_path}")

    pdata = sku_data_raw[sku]
    product = ProductMeta(
        sku=sku,
        name=pdata.get("name"),
        subtype=pdata.get("subtype"),
        type=pdata.get("type"),
        reviews=pdata.get("reviews", []),
        num_reviews=pdata.get("num_reviews", 0),
    )

    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º GOLDEN_TAGS
    name_tags, subtype_tags, type_tags = load_golden_tags_from_json(golden_tags_path_obj)

    # 3. –§–∏–ª—å—Ç—Ä—É–µ–º –æ—Ç–∑—ã–≤—ã
    filtered_reviews: List[str] = []
    for r in product.reviews:
        r = str(r).strip()
        if len(r) < min_review_length:
            continue
        if len(r) > max_chars:
            r = r[:max_chars]
        filtered_reviews.append(r)

    if not filtered_reviews:
        return []

    # 4. –ö–∞–Ω–¥–∏–¥–∞—Ç—ã —Ç–µ–≥–æ–≤
    candidate_tags = get_candidate_tags_for_product(
        name=product.name,
        subtype=product.subtype,
        product_type=product.type,
        name_tags=name_tags,
        subtype_tags=subtype_tags,
        type_tags=type_tags,
    )

    if not candidate_tags:
        return []

    # 5. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
    vectorizer = TagVectorizer(
        model_name=model_name,
        device=device,
        max_tags=max_tags,
    )

    tags = vectorizer.infer_tags_for_product(
        reviews=filtered_reviews,
        candidate_tags=candidate_tags,
        max_reviews=max_reviews,
    )

    # safety: –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –æ–±—Ä–µ–∂–µ–º –µ—â—ë —Ä–∞–∑
    return tags[:max_tags]
