# app.py
import traceback
from pathlib import Path
from typing import List, Dict, Any, Tuple

import streamlit as st
import numpy as np

from src.vector_pipeline import (
    TagVectorizer,
    load_golden_tags_from_json,
    get_candidate_tags_for_product,
)
from src.data_loader import load_dataset


# ------------------------
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã / –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
# ------------------------

DEFAULT_CSV_PATH = "/Users/macbook/bootcamp-lamoda/data/lamoda_reviews.csv"
DEFAULT_GOLDEN_PATH = "data/golden_tags_2_cleaned.json"
MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

MAX_TAGS = 6
MAX_CHARS = 500
MAX_REVIEWS = 50
MIN_REVIEW_LENGTH = 50


# ------------------------
# –ö—ç—à–∏
# ------------------------

@st.cache_data(show_spinner=False)
def cached_load_dataset(csv_path: str) -> Dict[str, Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ SKU (–∫—ç—à–∏—Ä—É–µ—Ç—Å—è)."""
    return load_dataset(csv_path=csv_path, min_reviews_per_sku=1)


@st.cache_data(show_spinner=False)
def cached_load_golden_tags(golden_path: str):
    """–ó–∞–≥—Ä—É–∑–∫–∞ GOLDEN_TAGS (–∫—ç—à–∏—Ä—É–µ—Ç—Å—è)."""
    return load_golden_tags_from_json(Path(golden_path))


@st.cache_resource(show_spinner=False)
def cached_vectorizer(model_name: str) -> TagVectorizer:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤–æ–π –º–æ–¥–µ–ª–∏."""
    return TagVectorizer(
        model_name=model_name,
        max_tags=MAX_TAGS,
    )


# ------------------------
# –õ–æ–≥–∏–∫–∞ –¥–ª—è –æ–¥–Ω–æ–π SKU
# ------------------------

def get_tags_and_evidence_for_sku(
    sku: str,
    csv_path: str,
    golden_tags_path: str,
    max_chars: int = MAX_CHARS,
    max_reviews: int = MAX_REVIEWS,
    min_review_length: int = MIN_REVIEW_LENGTH,
    max_tags: int = MAX_TAGS,
    top_reviews_per_tag: int = 5,
) -> Tuple[List[str], Dict[str, List[Tuple[str, float]]], Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      - —Å–ø–∏—Å–æ–∫ —Ç–µ–≥–æ–≤,
      - —Å–ª–æ–≤–∞—Ä—å tag -> [(review, score), ...],
      - —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ–¥—É–∫—Ç–∞.
    """
    # 1) –î–∞–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º SKU
    sku_data = cached_load_dataset(csv_path)
    if sku not in sku_data:
        raise ValueError(f"SKU '{sku}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ CSV")

    pdata = sku_data[sku]
    product_meta = {
        "sku": sku,
        "name": pdata.get("name"),
        "subtype": pdata.get("subtype"),
        "type": pdata.get("type"),
        "num_reviews": pdata.get("num_reviews", 0),
    }

    # 2) –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∑—ã–≤–æ–≤
    raw_reviews: List[str] = pdata.get("reviews", [])
    filtered_reviews: List[str] = []
    for r in raw_reviews:
        r = str(r).strip()
        if len(r) < min_review_length:
            continue
        if len(r) > max_chars:
            r = r[:max_chars]
        filtered_reviews.append(r)

    if not filtered_reviews:
        return [], {}, product_meta

    if len(filtered_reviews) > max_reviews:
        filtered_reviews = filtered_reviews[:max_reviews]

    # 3) GOLDEN_TAGS
    name_tags, subtype_tags, type_tags = cached_load_golden_tags(golden_tags_path)

    candidate_tags = get_candidate_tags_for_product(
        name=product_meta["name"],
        subtype=product_meta["subtype"],
        product_type=product_meta["type"],
        name_tags=name_tags,
        subtype_tags=subtype_tags,
        type_tags=type_tags,
    )

    if not candidate_tags:
        return [], {}, product_meta

    # 4) –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä
    vectorizer = cached_vectorizer(MODEL_NAME)

    # –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ç–µ–≥–æ–≤ (–æ–∫–Ω–∞ –ø–æ 4 —Å–ª–æ–≤–∞ –∏ —Ç.–ø.)
    tags = vectorizer.infer_tags_for_product(
        reviews=filtered_reviews,
        candidate_tags=candidate_tags,
        max_reviews=max_reviews,
    )

    tags = tags[:max_tags]

    # 5) –ü–æ–¥–±–æ—Ä "–æ–±—ä—è—Å–Ω—è—é—â–∏—Ö" –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–≥–∞
    #    –ó–¥–µ—Å—å –ø—Ä–æ—â–µ: —Å—á–∏—Ç–∞–µ–º —Å–∏–º–∏–ª—è—Ä–Ω–æ—Å—Ç—å –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º —Ç–µ–≥–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –ü–û–õ–ù–´–• –æ—Ç–∑—ã–≤–æ–≤.
    if not tags:
        return [], {}, product_meta

    review_embs = vectorizer.model.encode(
        filtered_reviews,
        convert_to_numpy=True,
        show_progress_bar=False,
    )  # (N_reviews, D)

    evidence: Dict[str, List[Tuple[str, float]]] = {}

    for tag in tags:
        tag_emb, _ = vectorizer._get_tag_embeddings(tag)  # –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã–π –∫–µ—à
        sims = vectorizer._cosine_sim(review_embs, tag_emb)  # (N_reviews,)

        # –±–µ—Ä–µ–º —Ç–æ–ø-N –æ—Ç–∑—ã–≤–æ–≤ –ø–æ —Å–∏–º–∏–ª—è—Ä–Ω–æ—Å—Ç–∏
        top_idx = np.argsort(-sims)[:top_reviews_per_tag]
        tag_evidence: List[Tuple[str, float]] = []
        for idx in top_idx:
            tag_evidence.append((filtered_reviews[idx], float(sims[idx])))

        evidence[tag] = tag_evidence

    return tags, evidence, product_meta


# ------------------------
# UI
# ------------------------

def main():
    st.set_page_config(
        page_title="Lamoda Tags Demo (vectorize)",
        page_icon="üëü",
        layout="wide",
    )

    st.title("Lamoda SKU ‚Üí —Ç–µ–≥–∏ (vectorize) üëü")
    st.markdown(
        """
–≠—Ç–æ –¥–µ–º–æ –±–µ–∑ LLM: —Ç–µ–≥–∏ –ø–æ–¥–±–∏—Ä–∞—é—Ç—Å—è –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º –∏ GOLDEN_TAGS.

**–ö–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è:**
1. –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ CSV —Å –æ—Ç–∑—ã–≤–∞–º–∏ –∏ –∫ —Ñ–∞–π–ª—É golden_tags.json  
2. –í–≤–µ–¥–∏ SKU  
3. –ù–∞–∂–º–∏ "–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–≥–∏"
        """
    )

    # --- –°–∞–π–¥–±–∞—Ä —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ ---
    st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")

    csv_path = st.sidebar.text_input(
        "–ü—É—Ç—å –∫ CSV —Å –æ—Ç–∑—ã–≤–∞–º–∏",
        value=DEFAULT_CSV_PATH,
    )
    golden_path = st.sidebar.text_input(
        "–ü—É—Ç—å –∫ GOLDEN_TAGS JSON",
        value=DEFAULT_GOLDEN_PATH,
    )
    max_chars = st.sidebar.number_input(
        "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞ (—Å–∏–º–≤–æ–ª—ã)",
        min_value=50,
        max_value=2000,
        value=MAX_CHARS,
        step=50,
    )
    max_reviews = st.sidebar.number_input(
        "–ú–∞–∫—Å–∏–º—É–º –æ—Ç–∑—ã–≤–æ–≤ –Ω–∞ SKU",
        min_value=5,
        max_value=200,
        value=MAX_REVIEWS,
        step=5,
    )
    min_review_length = st.sidebar.number_input(
        "–ú–∏–Ω. –¥–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–∞ (—Å–∏–º–≤–æ–ª—ã)",
        min_value=50,
        max_value=200,
        value=MIN_REVIEW_LENGTH,
        step=5,
    )
    top_reviews_per_tag = st.sidebar.number_input(
        "–°–∫–æ–ª—å–∫–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞ —Ç–µ–≥",
        min_value=1,
        max_value=20,
        value=2,
        step=1,
    )

    st.sidebar.markdown("---")
    st.sidebar.caption("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: " + MODEL_NAME)

    # --- –û—Å–Ω–æ–≤–Ω–æ–π –≤–≤–æ–¥ SKU ---
    st.subheader("–í–≤–æ–¥ SKU")
    sku = st.text_input(
        "–í–≤–µ–¥–∏—Ç–µ SKU —Ç–æ–≤–∞—Ä–∞",
        value="MP002XW0FXPS",
        placeholder="–ù–∞–ø—Ä–∏–º–µ—Ä: MP002XW0FXPS",
    )

    run_btn = st.button("–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–≥–∏")

    if run_btn:
        if not sku.strip():
            st.warning("–í–≤–µ–¥–∏—Ç–µ SKU.")
            return

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç–∏
        csv_path_abs = str(Path(csv_path).expanduser())
        golden_path_abs = str(Path(golden_path).expanduser())

        with st.spinner("–°—á–∏—Ç–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∏—â–µ–º —Ç–µ–≥–∏..."):
            try:
                tags, evidence, meta = get_tags_and_evidence_for_sku(
                    sku=sku.strip(),
                    csv_path=csv_path_abs,
                    golden_tags_path=golden_path_abs,
                    max_chars=max_chars,
                    max_reviews=max_reviews,
                    min_review_length=min_review_length,
                    max_tags=MAX_TAGS,
                    top_reviews_per_tag=top_reviews_per_tag,
                )
            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞: {e}")
                st.exception(e)
                st.text(traceback.format_exc())
                return

        if not tags:
            st.info("–¢–µ–≥–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–Ω–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ / –Ω–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ / –º–∞–ª–æ —Å–∏–≥–Ω–∞–ª–æ–≤).")
            return

        # --- –í—ã–≤–æ–¥ –æ–±—â–µ–π –∏–Ω—Ñ—ã –ø–æ –ø—Ä–æ–¥—É–∫—Ç—É ---
        st.markdown("### –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ç–æ–≤–∞—Ä–µ")
        cols = st.columns(4)
        cols[0].metric("SKU", meta.get("sku", "‚Äî"))
        cols[1].metric("–ù–∞–∑–≤–∞–Ω–∏–µ", meta.get("name", "‚Äî"))
        cols[2].metric("–ü–æ–¥—Ç–∏–ø", meta.get("subtype", "‚Äî"))
        cols[3].metric("–¢–∏–ø", meta.get("type", "‚Äî"))

        st.markdown(f"**–í—Å–µ–≥–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ SKU:** {meta.get('num_reviews', '‚Äî')}")

        # --- –¢–µ–≥–∏ ---
        st.markdown("### –¢–µ–≥–∏")
        st.write(", ".join(tags))

        st.markdown("### –û—Ç–∑—ã–≤—ã –ø–æ —Ç–µ–≥–∞–º")

        for tag in tags:
            st.markdown(f"#### üè∑Ô∏è {tag}")
            tag_reviews = evidence.get(tag, [])
            if not tag_reviews:
                st.write("_–ù–µ—Ç –æ—Ç–∑—ã–≤–æ–≤ –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ–≥–∞_")
                continue

            for i, (text, score) in enumerate(tag_reviews, start=1):
                with st.expander(f"–û—Ç–∑—ã–≤ {i} (similarity={score:.3f})", expanded=(i == 1)):
                    st.write(text)


if __name__ == "__main__":
    main()